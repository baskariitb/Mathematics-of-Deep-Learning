<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Perceptrons</title>
    <link rel="stylesheet" href="styles.css">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML ">
    </script>
    <style>
        body, p, li, div {
        color: black !important;
        line-height: 1.6;
        }
        ol.latex-enumerate, ol.latex-enumerate li {
            line-height: 1.6 !important;
            margin-top: 0.5em;
            margin-bottom: 0.5em;
        }
        /* Updated TOC styles */
        #toc-panel {
            font-size: 0.9em;
        }
        .toc-chapter, 
        .toc-section, 
        .toc-subsection {
            color: black !important;
        }
        .toc-chapter a,
        .toc-section a,
        .toc-subsection a {
            color: black !important;
            text-decoration: none;
        }
        .toc-chapter.current {
            background-color: rgba(26, 115, 232, 0.05);
        }
        #toc-button {
            position: fixed;
            top: 10px;
            left: 10px;
            background: #1a73e8;
            color: white;
            padding: 10px 15px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            z-index: 1000;
            box-shadow: 0 2px 5px rgba(0,0,0,0.2);
        }
        #toc-panel {
            position: fixed;
            top: 0;
            left: -350px;
            width: 300px;
            height: 100%;
            background: white;
            box-shadow: 2px 0 10px rgba(0,0,0,0.1);
            z-index: 999;
            padding: 20px;
            overflow-y: auto;
            transition: left 0.3s ease-out;
            will-change: transform;
        }
        #toc-panel.open {
            left: 0;
        }
        #close-toc {
            position: absolute;
            top: 10px;
            right: 10px;
            background: none;
            border: none;
            font-size: 20px;
            cursor: pointer;
        }
        #content {
            padding: 20px;
            padding-top: 60px;
            max-width: 100%;
            overflow-x: hidden;
            word-wrap: break-word;
        }
        .toc-chapter {
            margin-bottom: 10px;
            border-left: 3px solid #94714B;
            padding-left: 10px;
        }
        .toc-chapter.current {
            background-color: #F0EAE4;
        }
        .chapter-link {
            font-weight: bold;
            color: #1a73e8;
            text-decoration: none;
            display: block;
            padding: 5px 0;
        }
        .toc-sections {
            margin-left: 15px;
            display: none;
        }
        .toc-sections.expanded {
            display: block;
        }
        .toc-section {
           font-weight: bold;
            padding: 3px 0;
            margin-left: 10px;
        }
        .toc-subsection {
            padding: 1.5px 0;
            margin-left: 25px;
            font-size: 0.95em;
        }
        .toc-subsubsection {
            padding: 3px 0;
            margin-left: 40px;
            font-size: 0.9em;
            color: #555;
        }
        html {
            scroll-behavior: smooth;
        }
        :target {
            scroll-margin-top: 80px;
            animation: highlight 1.5s ease;
        }
        @keyframes highlight {
            0% { background-color: rgba(26, 115, 232, 0.1); }
            100% { background-color: transparent; }
        }
        @media (max-width: 768px) {
            #toc-panel {
                width: 85%;
                left: -90%;
            }
            #toc-panel.open {
                left: 0;
                box-shadow: 4px 0 15px rgba(0,0,0,0.2);
            }
            #content {
                transition: transform 0.3s ease-out;
            }
            #toc-panel.open ~ #content {
                transform: translateX(85%);
            }
        }
        .note {
  background-color: color-mix(in srgb, var(--primary) 7%, white);
border: 5px solid var(--primary);
  padding: 5px;
  margin: 10px;
}
.problem {
  background-color: color-mix(in srgb, var(--primary) 18%, white);
border: 1px solid var(--primary);
  padding: 10px;
  margin: 10px;
}
.example {
  background-color: var(--environmentBackground);
border-top: 7px solid var(--primary);
border-bottom: 7px solid var(--primary);
border-left: none;
border-right: none;
  padding: 10px;
  margin: 10px;
}
.remark {
  background-color: var(--environmentBackground);
border-top: 7px solid var(--primary);
border-bottom: 7px solid var(--primary);
border-left: none;
border-right: none;
  padding: 20px;
  margin: 10px;
}
.definition {
  background-color: color-mix(in srgb, var(--primary) 10%, white);
border: 3px double var(--primary);
  padding: 10px;
  margin: 10px;
}
.lemma {
  background-color: color-mix(in srgb, var(--primary) 10%, white);
border: 3px solid var(--primary);
  padding: 10px;
  margin: 10px;
  box-shadow: 0px 0px 10px rgba(0,0,0,0.1);
}
.theorem {
  background-color: color-mix(in srgb, var(--primary) 10%, white);
border: 3px solid var(--primary);
  padding: 10px;
  margin: 10px;
  box-shadow: 0px 0px 10px rgba(0,0,0,0.1);
}
.proofs {
  background-color: var(--environmentBackground);
border: 5px solid var(--primary);
  padding: 10px;
  margin: 10px;
}
.project {
  background-color: var(--environmentBackground);
border: 3px solid var(--primary);
  padding: 10px;
  margin: 10px;
}
.hint {
  background-color: color-mix(in srgb, var(--primary) 10%, white);
border: 3px solid var(--primary);
  padding: 10px;
  margin: 10px;
}
.code {
  background-color: color-mix(in srgb, var(--primary) 13%, white);
border: 3px solid var(--primary);
  padding: 10px;
  margin: 10px;
}
.algorithm {
  background-color: color-mix(in srgb, var(--primary) 5%, white);
border: 3px solid var(--primary);
  padding: 10px;
  margin: 10px;
}
    </style>
</head>


<body>
    <button id="toc-button">‚ò∞ Table of Contents</button>
    <div id="toc-panel">
        <button id="close-toc">√ó</button>
       
       <!-- Small Main Page Button -->
<div style="text-align:right; margin-bottom: 10px;">
  <a href="index.html" 
     style="
       background: var(--toc-primary, #94714B);
       color: white;
       padding: 4px 10px;
       border-radius: 16px;
       font-size: 0.75em;
       font-weight: 900;
       text-decoration: none;
       display: inline-block;
       box-shadow: 0 2px 6px rgba(0,0,0,0.12);
     ">
    Main Page
  </a>
</div>
       
        <div class="full-toc">

                <div class="toc-chapter ">
            <a href="HTMLIntroductionNeurons.html" class="chapter-link">
                CH 1: Introduction to Neurons
            </a>
             <div class="toc-sections ">

                <div class="toc-item ">
                    <a href="HTMLIntroductionNeurons.html#introduction.neurons.ch">
                        Introduction to Neurons
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLIntroductionNeurons.html#motivating.example.sec">
                        Motivating Example
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLIntroductionNeurons.html#water-source-sink-control-problem">
                        Water Source-Sink Control Problem
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLIntroductionNeurons.html#electric.circuit.ssec">
                        Electric Circuit
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLIntroductionNeurons.html#linear.regression.subs">
                        Linear Regression
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLIntroductionNeurons.html#artificial.neurons.sec">
                        Artificial Neurons
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLIntroductionNeurons.html#mathematical.formulation.subs">
                        Mathematical Formulation
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLIntroductionNeurons.html#comparison-with-biological-neurons">
                        Comparison with Biological Neurons
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLIntroductionNeurons.html#learning.model.subs">
                        Supervised Learning: An Overview
                    </a>
                </div>
</div></div>

        <div class="toc-chapter current">
            <a href="HTMLPerceptrons.html" class="chapter-link">
                CH 2: Perceptrons
            </a>
            <div class="toc-sections expanded">

                <div class="toc-item ">
                    <a href="HTMLPerceptrons.html#perceptrons.ch">
                        Perceptrons
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLPerceptrons.html#perceptrons.sec">
                        Neuron Model
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLPerceptrons.html#boolean-functions">
                        Boolean Functions
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLPerceptrons.html#illustrative-datasets">
                        Illustrative Datasets
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLPerceptrons.html#separability-and-algorithm">
                        Separability and Algorithm
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLPerceptrons.html#linear-separability">
                        Linear Separability
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLPerceptrons.html#training-algorithm">
                        Training Algorithm
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLPerceptrons.html#perceptron.convergence.theorem.subs">
                        Perceptron Convergence Theorem
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLPerceptrons.html#beyond-linear-separability">
                        Beyond Linear Separability
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLPerceptrons.html#feature-space-mapping">
                        Feature Space Mapping
                    </a>
                </div>
</div></div>

        <div class="toc-chapter ">
            <a href="HTMLSupportVectorMachine.html" class="chapter-link">
                CH 3: Support Vector Machine
            </a>
            <div class="toc-sections ">

                <div class="toc-item ">
                    <a href="HTMLSupportVectorMachine.html#support-vector-machine">
                        Support Vector Machine
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLSupportVectorMachine.html#linearly-separable-dataset">
                        Linearly Separable Dataset
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLSupportVectorMachine.html#hard-and-soft-margins">
                        Hard and Soft Margins
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLSupportVectorMachine.html#loss-function-form">
                        Loss-function form
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLSupportVectorMachine.html#subgradient-descent-algorithm">
                        Subgradient Descent Algorithm
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLSupportVectorMachine.html#dual-optimazation-problem">
                        Dual Optimazation Problem
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLSupportVectorMachine.html#nonlinearly-separable-datasets">
                        Nonlinearly Separable Datasets
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLSupportVectorMachine.html#kernel-method-implicit-feature-mapping">
                        Kernel Method: Implicit Feature Mapping
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLSupportVectorMachine.html#kernel-trick">
                        Kernel Trick
                    </a>
                </div>
</div></div>

        <div class="toc-chapter ">
            <a href="HTMLMultilayerPerceptrons.html" class="chapter-link">
                CH 4: Multilayer Perceptrons
            </a>
            <div class="toc-sections ">

                <div class="toc-item ">
                    <a href="HTMLMultilayerPerceptrons.html#multilayer.perceptrons.ch">
                        Multilayer Perceptrons
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLMultilayerPerceptrons.html#fully-connected-feedforward-networks">
                        Fully Connected Feedforward Networks
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#definition-and-network-dimensions">
                        Definition and Network Dimensions
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#shallow-networks">
                        Shallow Networks
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#batch-forward-propagation">
                        Batch Forward Propagation
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#batch-propagation">
                        Batch Propagation
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLMultilayerPerceptrons.html#activation-functions">
                        Activation Functions
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#step-functions">
                        Step Functions
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#heaviside-function">
                        Heaviside function
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#bipolar-function">
                        Bipolar Function
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#sigmoid-functions-logistic-regression">
                        Sigmoid Functions: Logistic Regression
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#hyperbolic-tangent-function">
                        Hyperbolic Tangent Function
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#bumped-type-functions">
                        Bumped-type Functions
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#hockey-stick-functions">
                        Hockey-stick Functions
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#leaky-rectified-linear-unit">
                        Leaky Rectified Linear Unit
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#sigmoid-linear-units">
                        Sigmoid Linear Units
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#softplus-function">
                        Softplus Function
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#multi-class-classification">
                        Multi-class Classification
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#softmax-function">
                        Softmax Function
                    </a>
                </div>
</div></div>

        <div class="toc-chapter ">
            <a href="HTMLTrainingNeuralNetwork.html" class="chapter-link">
                CH 5: Training Neural Network
            </a>
            <div class="toc-sections ">

                <div class="toc-item ">
                    <a href="HTMLTrainingNeuralNetwork.html#learning.mechanism.ch">
                        Training Neural Networks
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLTrainingNeuralNetwork.html#cost.functions.sec">
                        Cost Functions
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#mean-square-error">
                        Mean Square Error
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#cross-entropy">
                        Cross-Entropy
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#kullback-leibler-divergence">
                        Kullback-Leibler divergence
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLTrainingNeuralNetwork.html#backpropagation.sec">
                        Backpropagation Algorithm
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#chain-rule-and-gradient-computation">
                        Chain Rule and Gradient Computation
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#gradient-for-a-neuron">
                        Gradient for a Neuron
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#shallow-network">
                        Shallow Network
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#deep-network">
                        Deep Network
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLTrainingNeuralNetwork.html#weight-updates-using-gradient-descent">
                        Weight Updates Using Gradient Descent
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#batch-gradient-descent-method">
                        Batch Gradient Descent Method
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#stochastic-gradient-descent-sgd">
                        Stochastic Gradient Descent (SGD)
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#mini-batch-gradient-descent">
                        Mini-batch Gradient Descent
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#momentum-method">
                        Momentum Method
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#adaptive-step-size-methods">
                        Adaptive Step Size Methods
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#adagrad">
                        AdaGrad.
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#root-mean-square-propagation-rmsprop">
                        Root Mean Square Propagation (RMSProp)
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#adaptive-moments-adam">
                        Adaptive Moments (Adam).
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLTrainingNeuralNetwork.html#regularization-and-generalization">
                        Regularization and Generalization
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#underfitting-and-overfitting">
                        Underfitting and Overfitting
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#parameter-based-regularization">
                        Parameter-based Regularization
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#process-based-regularization">
                        Process-based Regularization
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#early-stopping">
                        Early Stopping
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#dropout">
                        Dropout
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#generalization">
                        Generalization
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#bias-variance-tradeoff">
                        Bias-Variance Tradeoff
                    </a>
                </div>
</div></div>

        <div class="toc-chapter ">
            <a href="HTMLExpressivityoverFunctionSpaces.html" class="chapter-link">
                CH 6: Expressivity Over Function Spaces
            </a>
            <div class="toc-sections ">

                <div class="toc-item ">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#expressivity.over.function.spaces.ch">
                        Expressivity over Function Spaces
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#universal.approximation.sec">
                        Function Approximations
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#continuous-function-approximations">
                        Continuous Function Approximations
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#feature-mapping-perspective">
                        Feature Mapping Perspective
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#connection-to-kernel-machines">
                        Connection to kernel machines
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#learning-dynamics-and-the-neural-tangent-kernel">
                        Learning dynamics and the Neural Tangent Kernel
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#pinns.sec">
                        Physics Informed Neural Networks for ODEs
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#trial-solution">
                        Trial Solution
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#residual-and-training-loss">
                        Residual and Training Loss
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#training-algorithm">
                        Training Algorithm
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#research-perspectives">
                        Research Perspectives
                    </a>
                </div>
</div></div>

        <div class="toc-chapter ">
            <a href="HTMLSpecialArchitectures.html" class="chapter-link">
                CH 7: Special Architectures
            </a>
            <div class="toc-sections ">

                <div class="toc-item ">
                    <a href="HTMLSpecialArchitectures.html#advanced.architectures.ch">
                        Special Architectures
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLSpecialArchitectures.html#convolution.neural.network.sec">
                        Convolution Neural Network
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLSpecialArchitectures.html#networks-of-one-dimensional-signals">
                        Networks of One-Dimensional Signals
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLSpecialArchitectures.html#one-dimensional-convolutional-layer">
                        One-Dimensional Convolutional Layer
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLSpecialArchitectures.html#two-dimensional-convolutional-layer">
                        Two-Dimensional Convolutional Layer
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLSpecialArchitectures.html#convolution-operator">
                        Convolution Operator
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLSpecialArchitectures.html#2d-convolution-layer">
                        2D Convolution Layer
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLSpecialArchitectures.html#recurrent-neural-networks">
                        Recurrent Neural Networks
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLSpecialArchitectures.html#transformers">
                        Transformers
                    </a>
                </div>
</div></div>
</div>
    </div>
    <div id="content">
        <p>
<h1 id="perceptrons.ch">Perceptrons</h1></p><p>In the previous chapter, we introduced the concept of an artificial neuron and outlined the supervised learning algorithm.  Artificial neurons (ANs) are the building blocks for artificial neural networks (ANNs).  A deeper understanding of ANs is therefore essential before proceeding further.  </p><p>Perceptrons are the simplest and most fundamental among various types of artificial neurons. They are binary classifiers designed to classify datasets that are <b>linearly separable</b>, where the decision boundary is a hyperplane. In such cases, a simple learning algorithm can be devised to determine the model parameters. Moreover, convergence of the learning algorithm can be analyzed using simple mathematical arguments. This chapter is devoted to a detailed discussion on perceptrons that includes a multi-epoch perceptron learning algorithm and its convergence.</p><p><a href="HTMLPerceptrons.html#perceptrons.sec" class="internal-link">Section  &laquo;Click Here&raquo;</a> introduces perceptrons and illustrates the construction of perceptrons for certain Boolean functions. This section also includes the proof of the non-existence of a perceptron for the <code>XOR</code> function and motivates the notion of linear separability as a necessary and sufficient condition for the existence of a perceptron for a dataset. The mathematical notion of linear separability of a dataset is then introduced in <a href="HTMLPerceptrons.html#separability.algorithm.sec" class="internal-link">Section  &laquo;Click Here&raquo;</a>, where a discussion of the perceptron learning algorithm and a proof of the perceptron convergence theorem are presented. Finally, <a href="HTMLPerceptrons.html#beyond.linear.separability.sec" class="internal-link">Section  &laquo;Click Here&raquo;</a> discusses the concept of feature mapping as a technique to handle datasets that are not linearly separable. We illustrate how mapping the input data to a higher-dimensional feature space can render it linearly separable, thereby enabling the construction of a perceptron in that feature space. </p><p><h2 id="perceptrons.sec">Neuron Model</h2></p><p>The ancient and the simplest artificial neuron is the one with the activation function as the Heaviside function or the step function given by
</p><div class="math" id="heaviside.function.eq">\begin{eqnarray}
H(x) = \left\{\begin{array}{lc}
		0,&amp;\text{if}~x<0\\
		1,&amp;\text{if}~x\ge 0
	\end{array}\right.
\end{eqnarray}<div style="text-align:right;">(2.1)</div></div><p>
Such an artificial neuron is called a <b>perceptron</b>, which we already illustrated in <a href="HTMLPerceptrons.html#AN.example">Example &laquo;Click Here&raquo; </a>. Perceptrons are oversimplified models that resemble biological neuron, where if the weighted input exceeds a threshold or bias <span class="math">\(b\)</span>, then the neuron gets activated. </p><p><div class="note" id="bipolar.function.eq">
<div class="heading-container">
<b class="heading">Note:</b>
</div>
One can also use the bipolar step function as the activation function, which is defined as
</p><div class="math" >\begin{eqnarray}
H(x) = \left\{\begin{array}{rc}
		-1,&amp;\text{if}~x<0\\
		1,&amp;\text{if}~x\ge 0
	\end{array}\right.
\end{eqnarray}<div style="text-align:right;">(2.2)</div></div><p>
Note that the perceptron theory we discuss here holds for both types of activation functions with some obvious modifications.
</div></p><p><div class="remark" id="existence.perceptron.rem">
<div class="heading-container">
<b class="heading">Remark:</b>
</div>

We say that a perceptron exists for a given dataset <span class="math">\(\mathcal{D} = \{(\boldsymbol{x}_k, y_k)~|~k=1,2,\ldots, N\} \subset \mathbb{R}^n \times \{0, 1\}\)</span> if there exists a <span class="math">\(\overline{\boldsymbol{w}}=(b,\boldsymbol{w}) \in \mathbb{R}\times \mathbb{R}^{n}\)</span>, where <span class="math">\(b\in \mathbb{R}\)</span> is a bias and <span class="math">\(\boldsymbol{w}=(w_1,w_2,\ldots, w_n)\in \mathbb{R}^n\)</span> is a weight vector, such that the resulting neuron <span class="math">\((\overline{\boldsymbol{w}}, H)\)</span> exactly models the dataset.  That is, the resulting neuron function <span class="math">\(\text{ùïó}\)</span> satisfies <span class="math">\(\text{ùïó}(\boldsymbol{x}_k) = y_k\)</span> for every <span class="math">\(k=1,2,\ldots, N\)</span>. Sometime, we also say that a perceptron can be learnt or trained if it exists for a dataset.
</div></p><p><h3 id="boolean-functions">Boolean Functions</h3></p><p>A Boolean function of <span class="math">\(n\)</span> variables is a function that takes an <span class="math">\(n\)</span>-dimensional input vector <span class="math">\(\boldsymbol{x}\in \{0,1\}^n\)</span> and produces a binary output digit <span class="math">\(y\in \{0,1\}\)</span>. </p><p>Logical functions are examples of Boolean functions. Perceptrons can be used to represent some logical functions as illustrated below:</p><p><div class="example" id="ANDPerceptron.ex">
<div class="heading-container">
<b class="heading">Example:</b>
</div>

Consider the 2-dimensional dataset for logical <code>AND</code> function given in the following truth table:
<img src="Figures/Ch02AND.png" class="standalone-figure">
It is straight forward to model a perceptron <span class="math">\((\overline{\boldsymbol{w}}, H)\)</span> for the logical <code>AND</code> function. In fact, any choice of the weights <span class="math">\(w_1\)</span> and <span class="math">\(w_2\)</span>, and the bias <span class="math">\(w_0=b\)</span> such that
</p><div class="math">\[
b \left\{ \begin{array}{ll}< x_1w_1 + x_2 w_2 , &amp;\text{if}~x_1=x_2=1\\
					 >x_1w_1 + x_2 w_2 , &amp;\text{otherwise}.
		\end{array}\right.
\]</div><p>
will be a perceptron that generates the same output as the <span class="math">\(\land\)</span> operator.  Here the line <span class="math">\(x_1w_1 + x_2 w_2 = b\)</span> is referred to as a <b>decision boundary</b>.</p><p>For instance, we can choose <span class="math">\(\overline{\boldsymbol{w}}=(1.5,1, 1)\)</span>. The figure below depicts the decision boundary <span class="math">\(x_1+x_2 = 1.5\)</span> of the perceptron with the neuron function <span class="math">\(\text{ùïó}(x_1,x_2) = H(x_1+x_2-1.5).\)</span>
</div></p><p><div class="figure">

<img src="Figures/Ch02ANDFig.png" class="standalone-figure">
<div style="text-align: center; font-weight: bold;">Input vectors and decision boundary (blue) of a perceptron for the <span class="math">\(\land\)</span> operator.</div>

</div></p><p><div class="problem">
<div class="heading-container">
<b class="heading">Problem:</b>
</div>
Give the truth table for the <code>OR</code> gate function <span class="math">\(\lor: \{0,1\}^2\rightarrow \{0,1\}\)</span>. Considering the truth table as the dataset, provide the general form of a perceptron that models the <code>OR</code> function and give the corresponding neuron function. Choose a particular perceptron and draw the diagram in the <span class="math">\((x_1,x_2)\)</span>-plane indicating the input data points, the decision boundary, and the regions corresponding to output values 0 and 1,  for the chosen perceptron.
</div></p><p>For <span class="math">\(n=2\)</span>, there are \( 2^4 = 16 \) possible Boolean functions.
Each Boolean function \( f: \{0,1\}^2 \to \{0,1\} \) maps every pair \( (x_1, x_2) \) to either 0 or 1. The list of all Boolean functions are given in the following table.
<img src="Figures/Ch02BooleanTable.png" class="standalone-figure">
Each column \( f_i \) corresponds to a Boolean function identified by the binary pattern in that column, interpreted as the output for inputs \((0,0), (0,1), (1,0), (1,1)\).</p><p>A few named Boolean functions are as follows:

  <li>\( f_0(x_1, x_2) = 0 \) (False)
  </li><li>\( f_1(x_1, x_2) = x_1 \land x_2 \) (<code>AND</code>)
  </li><li>\( f_6(x_1, x_2) = x_1 \oplus x_2 \) (<code>XOR</code> <span class="math">\(\rightarrow\)</span> exclusive <code>OR</code>)
  </li><li>\( f_7(x_1, x_2) = x_1 \lor x_2 \) (<code>OR</code>)
  </li><li>\( f_8(x_1, x_2) = \neg (x_1 \lor x_2) \) (<code>NOR</code>)
  </li><li>\( f_9(x_1, x_2) = \neg (x_1 \oplus x_2) \) (<code>XNOR</code>)
  </li><li>\( f_{14}(x_1, x_2) = \neg (x_1 \land x_2) \) (<code>NAND</code>)
  </li><li>\( f_{15}(x_1, x_2) = 1 \) (True)
</p><p>The following problem shows that not all Boolean functions can be represented by a perceptron.</p><p><div class="problem" id="XORPerceptron.prob">
<div class="heading-container">
<b class="heading">Problem:</b>
</div>

Show that there does not exist a perceptron for the <code>XOR</code> function.
</div></p><p><div class="hint">
<div class="heading-container">
<b class="heading">Hint:</b>
</div>
There are two ways in which we can prove the non-existence of a perceptron for the <code>XOR</code> function. One is a geometric approach, and the other is an algebraic approach. See page 139 in the following book:</p><p>Calin, Ovidiu, <i>Deep Learning Architectures: A Mathematical Approach</i>, Springer,	2020.</p><p><a href="https://link.springer.com/book/10.1007/978-3-030-36721-3" style="color: #008080; font-family: monospace;">
  Click here to see the details of the book
</a>
</div></p><p><h3 id="illustrative-datasets">Illustrative Datasets</h3></p><p>The above problem shows that perceptrons cannot learn every dataset. 
More precisely, if the decision boundary in a dataset is not a hyperplane, then there does not exists a perceptron that exactly classifies the data.</p><p>In this subsection, we present two examples: one illustrating a linearly separable dataset which has a linear decision boundary, and another where the classes are not linearly separable.</p><p>We begin with a practical scenario where the data are separated by a linear decision boundary.</p><p><div class="figure">

<img src="Figures/Ch02PlacementProblem.png" class="standalone-figure">
<div style="text-align: center; font-weight: bold;">(a) The full dataset; (b) the training dataset for the example given below. Green dots indicate students who were successfully placed, while red dots represent those who were not placed. There are 250 examples in the full dataset and 70% of examples are taken randomly into the training dataset.</div>

</div></p><p><div class="example" id="placement.training.dataset.ex">
<div class="heading-container">
<b class="heading">Example:</b>
</div>
[<b> Placement Prediction System: Synthetic Dataset Generation</b>]</p><p>The placement office in an educational institute has a dataset of students who appeared for the placement for the past five years. Let us hypothetically assume that a student's success in getting a job depends only on the following two factors:
<ol class="latex-enumerate">
</li><li>the CPI, denoted by <span class="math">\(x_1\)</span>; and
</li><li>the marks scored in the screening test, denoted by <span class="math">\(x_2\)</span>.
</li></ol>
The success in getting a job is denoted by <span class="math">\(y=1\)</span> and the failure is taken as <span class="math">\(y=0\)</span>.  The aim is to train a perceptron model to predict whether a student will get a job or not based on the values of <span class="math">\((x_1, x_2)\)</span> of that student.  </p><p>Recall that a perceptron is represented by the tuple <span class="math">\((\overline{\boldsymbol{w}}, H)\)</span>, where <span class="math">\(\overline{\boldsymbol{w}}=(b,w_1,w_2)\)</span> is a weight vector to be obtained. Finding a weight vector <span class="math">\(\overline{\boldsymbol{w}}\)</span> is referred to as learning (or training) a model. We have already outlined the learning algorithm for a neuron in the previous chapter. As per the learning algorithm, we first have to get a dataset <span class="math">\(\mathcal{D}_\text{train}\)</span>, called a <b>training set</b>, to train the model. </p><p>Assume that the placement office provided us with their dataset <span class="math">\(\mathcal{D}\)</span>, which is shown in <a href="HTMLPerceptrons.html#placement.training.dataset.fig">Figure &laquo;Click Here&raquo; </a>(a) and a training dataset  <span class="math">\(\mathcal{D}_\text{train}\)</span> is depicted in <a href="HTMLPerceptrons.html#placement.training.dataset.fig">Figure &laquo;Click Here&raquo; </a>(b).  The decision boundary is shown as the black dashed line. Since the decision boundary is a straight line, we can train a perceptron for this dataset.
</div></p><p><div class="code">
<div class="heading-container">
<b class="heading">Code:</b>
</div>
Understand the following python code that generates the above figure:

<a href="https://colab.research.google.com/drive/1LGaOpNz3zF_ccyqFc24tPaWZveJmGO5f?usp=sharing">
PlacementPredictorDatasetGenerator
</a>
</div></p><p>We will see in the next section that the dataset illustrated in the above example is a <b>linearly separable dataset</b>.  </p><p>Our next example illustrates a practical scenario where the dataset is not linearly separable.</p><p><div class="figure">

<img src="Figures/Ch02CreditCardProblem.png" class="standalone-figure">
<div style="text-align: center; font-weight: bold;">(a) The full dataset; (b) the training dataset for the example given below. Green dots represent approved applications and red dots indicate declined applications. There are 500 examples in the full dataset and 70% of examples are taken randomly into the training dataset.</div>


</div></p><p><div class="example" id="creditcard.training.dataset.ex">
<div class="heading-container">
<b class="heading">Example:</b>
</div>
[<b>Credit Card Approval Decision System: Synthetic Dataset Generation</b>]
</p><p>Assume that a bank issues credit cards, where they use various customer-related parameters for approval decisions.  Over the years, the bank has accumulated a large database of credit card approval history, where humans were involved in taking the approval decisions based on certain internal policy rules.  Now, the bank aims to build a model to automate these decisions.</p><p>For simplicity, let us assume that the approval decision depends only on two parameters, namely, 
<ol class="latex-enumerate">
<li>the customer's monthly income, denoted by <span class="math">\(x_1\)</span>; and
</li><li>the customer's expected monthly expenditure through the credit card, denoted by <span class="math">\(x_2\)</span>.
</li></ol>
Let us consider the bank's dataset <span class="math">\(\mathcal{D}\)</span> as depicted in the above <a href="HTMLPerceptrons.html#bank.training.dataset.fig">Figure &laquo;Click Here&raquo; </a>(a). Further, a training dataset <span class="math">\(\mathcal{D}_\text{train}\)</span> chosen randomly from <span class="math">\(\mathcal{D}\)</span> is shown in the above <a href="HTMLPerceptrons.html#bank.training.dataset.fig">Figure &laquo;Click Here&raquo; </a>(b).</p><p>
</div></p><p><div class="code" id="credit.card.dataset.generator.cd">
<div class="heading-container">
<b class="heading">Code:</b>
</div>

Write a Python code to plot  a pair of figures similar to the above <a href="HTMLPerceptrons.html#bank.training.dataset.fig">Figure &laquo;Click Here&raquo; </a> by first generating a synthetic dataset <span class="math">\(\mathcal{D}\)</span> and then randomly selecting 70% of the examples from <span class="math">\(\mathcal{D}\)</span> to form a training dataset <span class="math">\(\mathcal{D}_\text{train}\)</span>.  </p><p>Design your own decision boundary (that roughly resembles the observed boundary in the figure) by interpreting the following approval policy rules:
<ol class="latex-enumerate">
<li>Customers with high income and high credit card utilization are approved.
</li><li>Customers with low income and low credit card utilization are also approved.
</li></ol>
</div></p><p><h2 id="separability-and-algorithm">Separability and Algorithm</h2>
</p><p>In the previous section, we illustrated how to construct a perceptron for a given Boolean function.  Since the datasets in those examples contained only a few data points, it was easy to obtain the weights manually, which we refer to as constructing a model.  However, when the dataset is large, obtaining the weights manually may be computationally costly or even impossible. Therefore, it is desirable to have an efficient algorithm to compute the weights automatically. This can be achieved by first considering an appropriate cost function and then computing the weights that minimizes the cost function, which is referred to as <b>learning a model</b>. We discuss different ways of defining cost functions and some optimization methods in a later chapter.  In this section, we introduce a simple learning algorithm to train a perceptron based on incremental updates.</p><p><h3 id="linear-separability">Linear Separability</h3></p><p>First, let us define the mathematical notion of a linearly separable dataset.</p><p><div class="definition" id="linearly.separable.def">
<div class="heading-container">
<b class="heading">Definition:</b>
</div>
[<b>Linearly Separable</b>]</p><p>Two subsets <span class="math">\(\mathcal{C}^-\)</span> and <span class="math">\(\mathcal{C}^+\)</span> of <span class="math">\(\mathbb{R}^{n}\)</span> are said to be <b>linearly separable</b> if there exists a vector <span class="math">\(\overline{\boldsymbol{w}}=(w_0, w_1, \ldots, w_n) \in \mathbb{R}^{n+1}\)</span> such that
</p><div class="math" >\begin{eqnarray}
\begin{array}{l}
\text{for every } \boldsymbol{x}=(x_1, \ldots, x_n) \in \mathcal{C}^-, \text{ we have }
\displaystyle{\sum_{k=1}^n} w_k x_k < w_0; {and}\\
\text{for every } \boldsymbol{x}=(x_1, \ldots, x_n) \in \mathcal{C}^+, \text{ we have }
\displaystyle{\sum_{k=1}^n} w_k x_k \ge w_0.
\end{array}
\end{eqnarray}<div style="text-align:right;">(2.3)</div></div><p>
We call such a vector <span class="math">\(\overline{\boldsymbol{w}}\)</span> an <b>augmented weight vector</b>.
</div></p><p><div class="remark">
<div class="heading-container">
<b class="heading">Remark:</b>
</div>
[<b>Geometric Interpretation</b>]</p><p>Let us write the augmented weight vector as <span class="math">\(\overline{\boldsymbol{w}} = (w_0, \boldsymbol{w})\in \mathbb{R}\times \mathbb{R}^n\)</span>, where <span class="math">\(\boldsymbol{w}=(w_1,w_2,\ldots, w_n)\)</span>. If the sets <span class="math">\(\mathcal{C}^-\)</span> and <span class="math">\(\mathcal{C}^+\)</span> are linearly separable, then there exists a hyperplane 
</p><div class="math">\[
L: \boldsymbol{w}\cdot\boldsymbol{x} = w_0,
\]</div><p> 
called a <b>decision boundary</b>, that separates these two subsets. The weight vector <span class="math">\(\boldsymbol{w}\)</span> is normal (perpendicular) to <span class="math">\(L\)</span>.
</div></p><p>The following lemma provides a necessary and sufficient condition for the existence of a perceptron for a given dataset, which is a direct consequence of <a href="HTMLPerceptrons.html#existence.perceptron.rem">Remark &laquo;Click Here&raquo; </a> and <a href="HTMLPerceptrons.html#linearly.separable.def">Definition &laquo;Click Here&raquo; </a>.</p><p><div class="lemma" id="existence.perceptron.lem">
<div class="heading-container">
<b class="heading">Lemma:</b>
</div>
[<b>Existence of a Perceptron</b>] 
</p><p>A perceptron exists for a dataset <span class="math">\(\mathcal{D}\subset \mathbb{R}^{n}\times \{0,1\}\)</span> if and only if there exist two linearly separable sets <span class="math">\( \mathcal{C}^-\subset \mathbb{R}^{n}\)</span> and <span class="math">\(\mathcal{C}^+\subset \mathbb{R}^{n} \)</span> such that 
</p><div class="math">\[
\mathcal{D} = \big(\mathcal{C}^- \times \{0\}\big)  \cup  \big(\mathcal{C}^+ \times \{1\} \big).
\]</div><p>
</div></p><p><div class="remark">
<div class="heading-container">
<b class="heading">Remark:</b>
</div>
While discussing neurons, we use the notation <span class="math">\(\mathcal{D} = \mathcal{C}\times \mathbb{R}\)</span> to denote a dataset in  supervised learning.  Here, <span class="math">\(\mathcal{C}\)</span> denotes the set of input vectors, and <span class="math">\(\mathbb{R}\)</span> represents the corresponding outputs (or labels). </p><p>We call a dataset <b>linearly separable</b> if there exists a partition of the input set <span class="math">\(\mathcal{C}\)</span> consisting of two linearly separable sets. </p><p>Equivalently, by virtue of <a href="HTMLPerceptrons.html#linearly.separable.def">Definition &laquo;Click Here&raquo; </a>, we say that a dataset is linearly separable if there exists an augmented weight vector <span class="math">\(\overline{\boldsymbol{w}}\)</span> that classifies all examples of the dataset correctly.
</div></p><p><h3 id="training-algorithm">Training Algorithm</h3></p><p>Given a linearly separable dataset, our next task is to design an algorithm that learns a perceptron by computing a separating hyperplane. We begin by describing an algorithm for a single <b>epoch</b> in the training process using a given training dataset <span class="math">\(\mathcal{D}_\text{train}\)</span>.</p><p><div class="algorithm" id="single.epoch.perceptron.alg">
<div class="heading-container">
<b class="heading">Algorithm:</b>
</div>
[<b>Perceptron Learning (Single Epoch)</b>]
</p><p><b>Input:</b> 
<ol class="latex-enumerate">
<li>the training dataset 
</p><div class="math">\[
\mathcal{D}_\text{train}=\{(\boldsymbol{x}_k,y_k)~|~ k=1,2,\ldots, N_\text{train}\};
\]</div><p>
</li><li>the initial augmented weight vector <span class="math">\(\overline{\boldsymbol{w}}_0=(b_0, w_{0,1}, \ldots, w_{0,n})\in \mathbb{R}^{n+1}\)</span>; and
</li><li>the <b>learning rate</b> <span class="math">\(\eta \in (0,\infty)\)</span>.
</li></ol></p><p><b>Processing:</b></p><p>Let <span class="math">\(N_\text{train} = \#(\mathcal{D}_\text{train})\)</span> (cardinality of the set <span class="math">\(\mathcal{D}_\text{train}\)</span>).</p><p>For each <span class="math">\(k=1,2,\ldots, N_\text{train}\)</span>, perform the following <b>perceptron update rule</b>:
</p><div class="math">\[
\overline{\boldsymbol{w}}_k = \overline{\boldsymbol{w}}_{k-1} + \eta\Big( y_k - H(\overline{\boldsymbol{w}}_{k-1} \cdot \overline{\boldsymbol{x}}_k)\Big) \overline{\boldsymbol{x}}_k.
\]</div><p>
Recall, the augmented input vector is given by <span class="math">\(\overline{\boldsymbol{x}}_k = (-1, \boldsymbol{x}_k)\)</span>.</p><p><b>Output:</b> <span class="math">\(\overline{\boldsymbol{w}}^{(1)} := \overline{\boldsymbol{w}}_{N_\text{train}}\)</span>.</p><p>
</div></p><p><div class="remark">
<div class="heading-container">
<b class="heading">Remark:</b>
</div>
Observe from the above algorithm that the augmented weight vector <span class="math">\(\overline{\boldsymbol{w}}_N\)</span> is obtained by sequentially applying the update rule to all the examples in the training dataset. Such a complete pass through the entire training set is referred to as an <b>epoch</b>.
</div></p><p>Let us illustrate the computation of a single epoch by considering a small training dataset .</p><p><div class="example" id="single.epoch.example1.ex">
<div class="heading-container">
<b class="heading">Example:</b>
</div>

Consider the following training dataset <span class="math">\(\mathcal{D}_\text{train}\)</span>:
<img src="Figures/Ch02SingleEpochExample.png" class="standalone-figure">
Let us take <span class="math">\(\overline{\boldsymbol{w}}_0 = (0, 0, 0)\)</span> and <span class="math">\(\eta = 1\)</span>.</p><p>Since our training dataset contains three examples (<span class="math">\(N_{\text{train}}=3\)</span>), we have to perform three updates to obtain the augmented weight vector <span class="math">\(\overline{\boldsymbol{w}}^{(1)}\)</span> in one epoch.</p><p>The updates in a single epoch using <a href="HTMLPerceptrons.html#single.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a> are the following:</p><p><b>Update 1</b>:  <span class="math">\(\overline{\boldsymbol{w}}_1 = (1,-0.9,-0.1).\)</span></p><p><b>Update 2</b>: <span class="math">\(\overline{\boldsymbol{w}}_2 = (0, -0.8, 0.8).\)</span></p><p><b>Update 3</b>: <span class="math">\(\overline{\boldsymbol{w}}_3 = (0, -0.8, 0.8).\)</span></p><p>Therefore, the augmented weight vector after the single epoch is given by
</p><div class="math">\[
\overline{\boldsymbol{w}}^{(1)} = (0, -0.8, 0.8).
\]</div><p>
The perceptron learnt from the single epoch <a href="HTMLPerceptrons.html#single.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a> is <span class="math">\((\overline{\boldsymbol{w}}^{(1)},H)\)</span>. 
The decision lines along with the training dataset are depicted in the following figure.
</div></p><p><div class="figure">

<img src="Figures/Ch02SingleEpochExample1.png" class="standalone-figure">
<div style="text-align: center; font-weight: bold;">Decision lines for the updates in <a href="HTMLPerceptrons.html#single.epoch.example1.ex">Example &laquo;Click Here&raquo; </a>.</div>

</div></p><p><div class="problem">
<div class="heading-container">
<b class="heading">Problem:</b>
</div>
Perform one epoch for the following training dataset <span class="math">\(\mathcal{D}_\text{train}\)</span> with <span class="math">\(\overline{\boldsymbol{w}}_0 = (0, 0, 0)\)</span> and <span class="math">\(\eta = 1\)</span> to obtain the augmented weight vector <span class="math">\(\overline{\boldsymbol{w}}^{(1)}\)</span>:
<img src="Figures/Ch02SingleEpochExercise.png" class="standalone-figure">
Test the perceptron model (<span class="math">\(\overline{\boldsymbol{w}}^{(1)},H)\)</span> on the training set by directly applying the model to each example of the dataset. Give the conclusion about whether the trained perceptron represents the training dataset exactly or not.
</div></p><p><div class="code" id="single.epoch.code">
<div class="heading-container">
<b class="heading">Code:</b>
</div>

Understand the following Python code implementing <a href="HTMLPerceptrons.html#single.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a>:</p><p>
<a href="https://colab.research.google.com/drive/1AsheZBIF6Dl9P-ojlpqlQCppaIwMQp-W?usp=sharing">
SingleEpochPerceptron
</a>
</p><p>Modify the above Python code to include a visual representation (graph) of the given dataset along with the graph of the separating lines at each update.
</div></p><p>The datasets considered in the following problem is similar to the placement prediction problem considered in <a href="HTMLPerceptrons.html#placement.training.dataset.ex">Example &laquo;Click Here&raquo; </a>.</p><p><div class="problem">
<div class="heading-container">
<b class="heading">Problem:</b>
</div>
Consider the training dataset <span class="math">\(\mathcal{D}_\text{train}\)</span> given in the following table:
<img src="Figures/Ch02PlacementExercise.png" class="standalone-figure">
Take <span class="math">\(\overline{\boldsymbol{w}}_0=\boldsymbol{0}\)</span>, the zero vector, and <span class="math">\(\eta=0.9\)</span>. Perform one epoch to get <span class="math">\(\overline{\boldsymbol{w}}^{(1)}\)</span> using <a href="HTMLPerceptrons.html#single.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a>. Does the perceptron model <span class="math">\((\overline{\boldsymbol{w}}^{(1)}, H)\)</span> fit the training dataset exactly?
</div></p><p>Often, a single epoch is not sufficient to obtain a perceptron that exactly (or even approximately) fits the training dataset. In such cases, it is necessary to perform multiple epochs, by taking the output of one epoch as the initial weight vector for the next epoch, and updating the weights iteratively until a satisfactory perceptron is learned.</p><p>The following algorithm trains a perceptron using multiple epochs:</p><p><div class="algorithm" id="multiple.epoch.perceptron.alg">
<div class="heading-container">
<b class="heading">Algorithm:</b>
</div>
[<b>Perceptron Learning (Multiple Epoch)</b>]
</p><p><b>Input:</b> 
<ol class="latex-enumerate">
<li>the training dataset <span class="math">\(\mathcal{D}_\text{train}=\{(\boldsymbol{x}_k,y_k)~|~ k=1,2,\ldots, N_\text{train}\}\)</span>;
</li><li>the initial augmented weight vector <span class="math">\(\overline{\boldsymbol{w}}_0=(b_0, w_{0,1}, \ldots, w_{0,n})\in \mathbb{R}^{n+1}\)</span>; and
</li><li>the <b>learning rate</b> <span class="math">\(\eta \in (0,\infty)\)</span>.
</li></ol></p><p><b>Processing:</b>
Let <span class="math">\(N_\text{train} = \#(\mathcal{D}_\text{train})\)</span> (cardinality of the set <span class="math">\(\mathcal{D}_\text{train}\)</span>)
<ol class="latex-enumerate">
<li>Set <span class="math">\(\overline{\boldsymbol{w}}^{(0)} = \overline{\boldsymbol{w}}_0.\)</span>
</li><li>Define the augmented input vector <span class="math">\(\overline{\boldsymbol{x}}_k = (-1, \boldsymbol{x}_k)\)</span>, for all <span class="math">\(k=1,2,\ldots, N_\text{train}\)</span>.
</li><li>For each <span class="math">\(j=1,2,\ldots\)</span>, perform the following <b>single epoch</b>:

</li><li><b>Update:</b> For each <span class="math">\(k=1,2,\ldots, N_\text{train}\)</span>, perform the following update rule:

</li><li><b>Step 1:</b> Compute <span class="math">\(\Delta_k = y_k - H(\overline{\boldsymbol{w}}_{k-1} \cdot \overline{\boldsymbol{x}}_k).\)</span>
</li><li><b>Step 2:</b> If <span class="math">\(| \Delta_k| < 1\)</span>, then take <span class="math">\(\overline{\boldsymbol{w}}_k = \overline{\boldsymbol{w}}_{k-1}.\)</span> 
</li><li><b>Step 3:</b> Otherwise, use the <b>perceptron update rule</b>:
</p><div class="math">\[
\overline{\boldsymbol{w}}_k = \overline{\boldsymbol{w}}_{k-1} + \eta\Delta_k\, \overline{\boldsymbol{x}}_k.
\]</div><p>


</li><li><b>Assign:</b> Set <span class="math">\(\overline{\boldsymbol{w}}^{(j)} := \overline{\boldsymbol{w}}_{N_\text{train}}\)</span>.

</li><li><b>Check:</b> For <span class="math">\(j\ge 1\)</span>, check the following condition:
	</p><div class="math" >\begin{eqnarray}
		\|\overline{\boldsymbol{w}}^{(j)} - \overline{\boldsymbol{w}}^{(j-1)}\|_2 = 0. 
	\end{eqnarray}<div style="text-align:right;">(2.4)</div></div><p>
	<b>If</b> this condition is not satisfied, then take <span class="math">\(\overline{\boldsymbol{w}}_0 = \overline{\boldsymbol{w}}_{N_\text{train}}\)</span> and go for the next epoch (Point 3 above).</p><p>	<b>Else if</b> this condition is satisfied, then stop the computation.
</li></ol>
</ol></p><p><b>Output:</b> <span class="math">\(\overline{\boldsymbol{w}}^{(n_e)}\)</span> where <span class="math">\(n_e=j-1\)</span>, the number of epochs taken to get a desired weight.
</div></p><p><div class="note">
<div class="heading-container">
<b class="heading">Note:</b>
</div>
The norm <span class="math">\(\|\cdot\|_2\)</span> used in the check condition above is the euclidean norm.
</div></p><p><div class="remark">
<div class="heading-container">
<b class="heading">Remark:</b>
</div>
[<b>Perceptron Convergence</b>]</p><p>If the check condition in the above algorithm is satisfied for some <span class="math">\(j=1,2,\ldots\)</span>, we say that the <b>perceptron algorithm converged</b> in <span class="math">\(j-1\)</span> epochs.</p><p>The perceptron trained through the above multi-epoch algorithm is denoted by <span class="math">\((\overline{\boldsymbol{w}}^{(n_e)}, H)\)</span>, where <span class="math">\(n_e\)</span> denotes the number of epochs taken by the algorithm to converge.
</div></p><p><div class="code">
<div class="heading-container">
<b class="heading">Code:</b>
</div>
Extend the python program given in <a href="HTMLPerceptrons.html#single.epoch.code">Code &laquo;Click Here&raquo; </a> to implement the multiple epoch perceptron learning <a href="HTMLPerceptrons.html#multiple.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a>. Test your code with the following dataset:
<img src="Figures/Ch02PlacementCode.png" class="standalone-figure">
Take <span class="math">\(\overline{\boldsymbol{w}}=\boldsymbol{0}\)</span>, the zero vector, and <span class="math">\(\eta=0.9\)</span>. Find the number of epochs needed for the convergence of the weight vector sequence.  Let the weight vector limit be <span class="math">\(\overline{\boldsymbol{w}}^{*}\)</span>. Using the perceptron model <span class="math">\((\overline{\boldsymbol{w}}^{*}, H)\)</span>, compute the mean squared error on the test dataset.
</div></p><p><div class="problem" id="algorithm.convergence..pr">
<div class="heading-container">
<b class="heading">Problem:</b>
</div>

For a given <span class="math">\(\overline{\boldsymbol{w}}^{(0)}\)</span>, <span class="math">\(\eta>0\)</span> and a training set <span class="math">\(\mathcal{D}_\text{train}\)</span>, let <span class="math">\(\{\overline{\boldsymbol{w}}^{(j)} \mid j = 1, 2, \ldots\}\)</span> be a sequence generated by the multi-epoch <a href="HTMLPerceptrons.html#multiple.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a>.
Suppose that there exists an integer <span class="math">\(J\ge 0\)</span> such that the perceptron <span class="math">\((\overline{\boldsymbol{w}}^{(J)}, H)\)</span> correctly classifies all the examples of the training dataset <span class="math">\(\mathcal{D}_\text{train}\)</span>. Then, show that the algorithm converges in <span class="math">\(n_e = J\)</span> epochs.  Does the converse hold? Justify your answer.
</div></p><p><h3 id="perceptron.convergence.theorem.subs">Perceptron Convergence Theorem</h3></p><p>The following theorem provides a sufficient condition for the convergence of  the multi-epoch <a href="HTMLPerceptrons.html#multiple.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a> in finite epochs.</p><p><div class="theorem" id="perceptron.convergence.theorem.th">
<div class="heading-container">
<b class="heading">Theorem:</b>
</div>
[<b>Perceptron Convergence Theorem</b>]
</p><p>Let the training dataset <span class="math">\(\mathcal{D}_\text{train}=\{(\boldsymbol{x}_k,y_k)~|~ k=1,2,\ldots, N_\text{train}\}\)</span> be linearly separable, and let <span class="math">\(\overline{\boldsymbol{w}}^*\)</span> be an augmented weight vector such that the perceptron <span class="math">\((\overline{\boldsymbol{w}}^*, H)\)</span> classifies all examples of <span class="math">\(\mathcal{D}_\text{train}\)</span> correctly with <span class="math">\(\overline{\boldsymbol{w}}^*\cdot \overline{\boldsymbol{x}}_k \ne 0\)</span>, for all <span class="math">\(k=1,2,\ldots, N_\text{train}\)</span>.  Then, for any initial augmented vector <span class="math">\(\overline{\boldsymbol{w}}_0\)</span> such that <span class="math">\(\overline{\boldsymbol{w}}^*\cdot \overline{\boldsymbol{w}}_0\ge 0\)</span> and any learning rate <span class="math">\(\eta > 0\)</span>, there exists an integer <span class="math">\(n_e\ge 0\)</span> such that the perceptron <span class="math">\((\overline{\boldsymbol{w}}^{(n_e)}, H)\)</span> classifies all examples in <span class="math">\(\mathcal{D}_\text{train}\)</span> correctly, where <span class="math">\(\overline{\boldsymbol{w}}^{(n_e)}\)</span> is computed using the multi-epoch <a href="HTMLPerceptrons.html#multiple.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a>.</p><p>
</div></p><p><div class="proofs" id="lower.bound.perceptron.update.eq">
<div class="heading-container">
<b class="heading">Proof:</b>
</div>
Assume the contrary that the multi-epoch perceptron <a href="HTMLPerceptrons.html#multiple.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a>  performs epochs indefinitely.</p><p>Assume that the multi-epoch perceptron <a href="HTMLPerceptrons.html#multiple.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a> had performed <span class="math">\(n_e>0\)</span> number of epochs and not all examples are correctly classified by the vector <span class="math">\(\overline{\boldsymbol{w}}^{(n_e) }\)</span>. Further, assume that <span class="math">\(\overline{\boldsymbol{w}}^{(n_e) }\)</span> is obtained after <span class="math">\(m-1\)</span> updates.</p><p>Let <span class="math">\(k_1\in \{1,2,\ldots, N_\text{train}\}\)</span> be the smallest integer such that the example (<span class="math">\(\boldsymbol{x}_{k_1}, y_{k_1}\)</span>) is not classified correctly by <span class="math">\(\overline{\boldsymbol{w}}^{(n_e) }\)</span>. Then, the algorithm performs the <span class="math">\(m^\text{th}\)</span> update and obtains <span class="math">\(\overline{\boldsymbol{w}}_{m}\)</span> which satisfies (how?)
</p><div class="math">\[
\overline{\boldsymbol{w}}^*\cdot \overline{\boldsymbol{w}}_{m} \ge  \overline{\boldsymbol{w}}^*\cdot\overline{\boldsymbol{w}}_0 + m\eta \rho.
\]</div><p>
where
</p><div class="math">\[
\rho
= \min\Big\{|\overline{\boldsymbol{w}}^*\cdot \overline{\boldsymbol{x}}_k|~|~ k=1,2,\ldots, N_\text{train}\Big\}>0.\\
\]</div><p>
Using Cauchy-Schwarz inequality, we obtain
</p><div class="math" >\begin{eqnarray}
\|\overline{\boldsymbol{w}}_{m}\|^2 \ge \frac{\big( \overline{\boldsymbol{w}}^*\cdot\overline{\boldsymbol{w}}_0 + m\eta \rho \big)^2}{\|\overline{\boldsymbol{w}}^*\|^2}.
\end{eqnarray}<div style="text-align:right;">(2.5)</div></div><p>
On the other hand, since <span class="math">\(\overline{\boldsymbol{x}}_{k_1}\)</span> is not classified by <span class="math">\(\overline{\boldsymbol{w}}_{m-1}\)</span> correctly, we can show that (how?)
</p><div class="math" >\begin{eqnarray}
\|\overline{\boldsymbol{w}}_{m}\|^2 \le \|\overline{\boldsymbol{w}}_{0}\|^2 + m\eta^2 r^2.
\end{eqnarray}<div style="text-align:right;">(2.6)</div></div><p>
where <span class="math">\(r := \max\{\|\overline{\boldsymbol{x}}_{k}\|~|~k=1,2,\ldots, N_\text{train}\}\)</span>. The above two inequalities  leads to a contradiction to our assumption that the algorithm performs epochs indefinitely. Thus, the algorithm has to terminate in a finite number of epochs.  </p><p>Further, by equating the lower and the upper bounds of <span class="math">\(\|\overline{\boldsymbol{w}}_{m}\|^2 \)</span>, we can obtain the maximum number of updates, denoted by <span class="math">\(m^*>0\)</span> (justify the existence of a positive <span class="math">\(m^*\)</span>). If there is an example <span class="math">\(\overline{\boldsymbol{x}}_k\)</span> that has still not been classified correctly by <span class="math">\(\overline{\boldsymbol{w}}_{m^*}\)</span>, then the algorithm will proceed to the next update <span class="math">\(\overline{\boldsymbol{w}}_{m^*+1}\)</span>, which must satisfy the above two conditions. But, both these conditions do not hold simultaneously for <span class="math">\(m=m^*+1\)</span> (why?).  Therefore, the perceptron (<span class="math">\(\overline{\boldsymbol{w}}_{m^*},H)\)</span> has to classify all the examples correctly.</p><p>
</div></p><p><div class="note">
<div class="heading-container">
<b class="heading">Note:</b>
</div>
Note the difference between the number of epochs and the number of updates, as is apparent from the above proof.
</div></p><p><h2 id="beyond-linear-separability">Beyond Linear Separability</h2>
</p><p>In the previous section, we discussed a simple multi-epoch algorithm to train a perceptron to classify a linearly separable dataset. However, in real-world applications, we often encounter datasets that are not linearly separable. </p><p>There are two main scenarios where linear separability does not hold in a dataset:</p><p><ol class="latex-enumerate">
<li>When the decision boundary cannot be represented by a hyperplane (for example, circular or any curved decision boundaries). In such  cases, the dataset is said to be <b>nonlinearly separable</b> and the decision boundary is called the <b>nonlinear discriminants</b>. </p><p></li><li>When the classification task require multiple hyperplanes (or curved surfaces) to separate the classes. This occurs in problems with complex class structures (such as the XOR example discussed in <a href="HTMLPerceptrons.html#XORPerceptron.prob">Problem &laquo;Click Here&raquo; </a> and the dataset generated in the credit card approval  <a href="HTMLPerceptrons.html#creditcard.training.dataset.ex">Example &laquo;Click Here&raquo; </a>) or multi-class classification problems.
</li></ol></p><p>The perceptron learning algorithm is only guaranteed to converge when the dataset is linearly separable. In general, single-neuron models are insufficient to learn datasets with complex decision boundaries.  There are at least two broad approaches to handle this situation.  One is to combine multiple neurons with nonlinear activation functions to form a deep neural network, which will be the subject of discussion in later chapters.  The other involves alternative machine learning tool called <b>feature engineering</b>, which we will briefly explore in this section. </p><p>The underlying idea of feature engineering is to transform the input data into another space, called the <b>feature space</b>, where the dataset becomes linearly separable. In this section, we discuss the feature space transformation with a few illustrations.</p><p><h3 id="feature-space-mapping">Feature Space Mapping</h3>
</p><p>Even if a dataset is not linearly separable in the input space, it may be possible to transform the input vectors <span class="math">\(\boldsymbol{x} \in \mathbb{R}^n\)</span> into new representations <span class="math">\(\boldsymbol{\phi}(\boldsymbol{x}) \in \mathbb{R}^{\hat{n}}\)</span>, such that the equivalent transformed dataset becomes linearly separable.  The perceptron training <a href="HTMLPerceptrons.html#multiple.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a> can then be applied directly in the feature space  to learn a linear classifier for the transformed data (see the following figure).</p><p><div class="figure">

<img src="Figures/Ch02FeatureSpaceSchematic.png" class="standalone-figure">
<div style="text-align: center; font-weight: bold;">Schematic view of mapping input data to a feature space where a linear classifier can be applied.</div>


</div></p><p>The function <span class="math">\(\boldsymbol{\phi}:\mathbb{R}^n\rightarrow \mathbb{R}^{\hat{n}}\)</span>, with <span class="math">\(\hat{n}\ge n\)</span>, is a nonlinear map called the <b>feature function</b>. It maps the <b>input space</b> <span class="math">\(\mathbb{R}^n\)</span> into a <b>feature space</b> <span class="math">\(\mathbb{R}^{\hat{n}}\)</span>.  </p><p>We write <span class="math">\(\boldsymbol{\phi}(\boldsymbol{x}) = (\phi_1(\boldsymbol{x}), \phi_2(\boldsymbol{x}), \ldots, \phi_{\hat{n}}(\boldsymbol{x}))\)</span>, where each component <span class="math">\(\phi_k:\mathbb{R}^n\rightarrow \mathbb{R}\)</span>, for <span class="math">\(k=1,2,\ldots, \hat{n}\)</span>, is called a <b>basis function</b>. </p><p>Consider the augmented feature function <span class="math">\(\overline{\boldsymbol{\phi}}(\boldsymbol{x}) = (-1, \phi_1(\boldsymbol{x}), \phi_2(\boldsymbol{x}), \ldots, \phi_{\hat{n}}(\boldsymbol{x}))\)</span> and define the <b>generalized linear discriminant</b> as
</p><div class="math">\[
\text{ùïí}(\overline{\boldsymbol{\phi}}(\boldsymbol{x});\overline{\boldsymbol{w}}) = \sum_{k=0}^{\hat{n}} w_k \phi_k(\boldsymbol{x}) = \sum_{k=1}^{\hat{n}} w_k \phi_k(\boldsymbol{x}) - b,
\]</div><p>
for a given augmented weight vector <span class="math">\(\overline{\boldsymbol{w}}=(w_0,w_1,\ldots, w_{\hat{n}})\)</span> in the feature space <span class="math">\(\mathbb{R}^{\hat{n}}\)</span> with <span class="math">\(w_0=b\in \mathbb{R}\)</span> being the bias.</p><p>If such a feature map is constructed, then a neuron (such as a perceptron) can be trained on this transformed dataset using a linear classifing algorithm (such as the perceptron <a href="HTMLPerceptrons.html#multiple.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a>).</p><p>To have a better understanding of the feature space transformation, we illustrate the construction of feature maps for a given dataset provided the discriminant function is known explicitly.</p><p><div class="example" id="circular.decision.boundary.ex">
<div class="heading-container">
<b class="heading">Example:</b>
</div>
[<b>Circular Decision Boundary</b>]
</p><p>Consider a two-dimensional input <span class="math">\( \boldsymbol{x} = (x_1, x_2) \)</span>. Let the nonlinear discriminant function in the input space be
</p><div class="math">\[
\text{ùïí}(\boldsymbol{x}) = x_1^2 + x_2^2 - r^2,
\]</div><p>
where <span class="math">\( r > 0 \)</span> is a given real number. The function <span class="math">\(\text{ùïí}\)</span> is nonlinear since it defines a circular decision boundary <span class="math">\(x_1^2 + x_2^2 = r^2\)</span>, where the region inside the circle corresponds to <span class="math">\(\text{ùïí}(\boldsymbol{x}) < 0 \)</span>, and the region outside corresponds to <span class="math">\(\text{ùïí}(\boldsymbol{x}) \ge 0 \)</span>.</p><p>In particular, consider the training dataset <span class="math">\(\mathcal{D}_{\text{train}} = \{(\boldsymbol{x}_k, y_k)\} \subset \mathbb{R}^2\times \{0,1\}\)</span> depicted in <a href="HTMLPerceptrons.html#Circular.Dataset.fig">Figure &laquo;Click Here&raquo; </a>(a). Here, we have taken <span class="math">\(r=1\)</span> and the decision boundary is the unit circle.  Obviously,  <a href="HTMLPerceptrons.html#multiple.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a> cannot be used to train a perceptron that represents this dataset correctly.</p><p>However, by mapping the training dataset into a suitable chosen feature space, we can train a perceptron.  For instance, choose <span class="math">\(\boldsymbol{\phi}(x_1,x_2) = (x_1^2, x_2^2)\)</span>, where the basis functions are <span class="math">\(\phi_k(x_1,x_2) = x_k^2\)</span>, <span class="math">\(k=1,2\)</span>.  Then, we can see that the generalized linear discriminant is
</p><div class="math">\[
\text{ùïí} = \overline{\boldsymbol{w}}\cdot \overline{\boldsymbol{\phi}},
\]</div><p>
where the augmented weight vector is <span class="math">\(\overline{\boldsymbol{w}} = (r^2, 1, 1)\)</span> and the augmented feature function is <span class="math">\(\overline{\boldsymbol{\phi}}=(-1, \phi_1, \phi_2)\)</span>. <a href="HTMLPerceptrons.html#multiple.epoch.perceptron.alg">Algorithm &laquo;Click Here&raquo; </a> can then be applied on the dataset in the <span class="math">\((\phi_1, \phi_2)\)</span>-plane, which is the feature space. The training dataset <span class="math">\(\mathcal{D}_{\text{train}} = \{(\boldsymbol{\phi}_k, y_k)\}\)</span> in the feature space is depicted in <a href="HTMLPerceptrons.html#Circular.Dataset.fig">Figure &laquo;Click Here&raquo; </a>(b).
</div></p><p><div class="figure">

<img src="Figures/Ch02CircularDataset.png" class="standalone-figure">
<div style="text-align: center; font-weight: bold;">An illustration of a nonlinearly separable dataset mapped to a linearly separable dataset with feature map discussed in <a href="HTMLPerceptrons.html#circular.decision.boundary.ex">Example &laquo;Click Here&raquo; </a>. Here, red dots represent labels with value <span class="math">\(0\)</span> and green dots correspond to labels with value +1. Black dashed line represents the decision boundaries in the respected spaces. (a) Dataset in the input space (b) Dataset in the feature space.</div>


</div></p><p>In the previous example, we illustrated an exact construction of the discriminant function, which is possible because the given function is a polynomial. However, if the discriminant function is non-polynomial, an explict feature space mapping may not be feasible. In this case, we have to approximate the discriminant function. We illustrate such a situation in the following problem.</p><p><div class="problem">
<div class="heading-container">
<b class="heading">Problem:</b>
</div>
Let the training dataset <span class="math">\(\mathcal{D}_\text{train}\in \mathbb{R}^2\times \{0,1\}\)</span> be such that the discriminant function is given by 
</p><div class="math">\[
\text{ùïí}(x_1, x_2)=e^{x_1^2 + x_2^2}.
\]</div><p> 
Using the Taylor expansion of <span class="math">\(\text{ùïí}\)</span> about the origin up to total degree 4, construct a feature mapping function <span class="math">\(\boldsymbol{\phi} : \mathbb{R}^2 \rightarrow \mathbb{R}^{\hat{d}}\)</span> such that the dataset, when mapped into the feature space <span class="math">\(\mathbb{R}^{\hat{d}}\)</span>, can be separated by a linear discriminant function. Also, obtain a suitable weight vector <span class="math">\(\boldsymbol{w}\)</span>.</p><p><b>Partial Answer:</b> <span class="math">\(\hat{d} = 6\)</span>
</div></p><p>Recall, in <a href="HTMLPerceptrons.html#XORPerceptron.prob">Problem &laquo;Click Here&raquo; </a>, we have seen that it is not possible to train a perceptron for the <code>XOR</code> function in the input space. However, by suitably choosing a feature map, we can train a perceptron for <code>XOR</code> in the feature space.</p><p><div class="problem">
<div class="heading-container">
<b class="heading">Problem:</b>
</div>
Obtain a suitable feature map <span class="math">\(\boldsymbol{\phi}\)</span> such that the <code>XOR</code> function \( f_6(x_1, x_2) = x_1 \oplus x_2 \) is represented exactly by a perceptron in the feature space.
</div></p><p>Building a feature map explicitly is seldom feasible, as the discriminant function is typically unknown and implicitly embedded in the dataset. It is therefore desirable to adapt methodologies in which the feature map is learned as part of the training process. This is a core idea in deep learning, where multiple layers of artificial neurons are used to extract increasingly abstract features from data. However, there are also other well-established machine learning models, such as Support Vector Machines (SVMs), that can learn complex decision boundaries directly from data through the use of kernels, without requiring an explicit construction of the feature map.</p>
    </div>
    
 <footer>
  ¬© S. Baskar, Department of Mathematics, IIT Bombay. 2025 ‚Äî Updated: 15-Aug-2025
</footer>

<style>
footer {
  position: fixed;
  bottom: 0;
  left: 0;
  width: 100%;
  background: #f8f8f8;
  color: #888;
  font-size: 0.75em;
  text-align: center;
  padding: 6px 0;
  border-top: 1px solid #ddd;
  z-index: 10;               /* low z-index */
  pointer-events: none;      /* allows clicks to pass through */
}

/* Small screen adjustments */
@media (max-width: 600px) {
  footer {
    font-size: 0.68em;
    padding: 4px 0;
  }
}
</style>

    <script>
    document.addEventListener("contextmenu", function(e) { e.preventDefault(); });
    document.addEventListener("keydown", function(e) {
        if ((e.ctrlKey || e.metaKey) && (e.key === "p" || e.key === "s")) {
            e.preventDefault();
        }
    });
    </script>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const tocButton = document.getElementById('toc-button');
            const tocPanel = document.getElementById('toc-panel');
            const closeToc = document.getElementById('close-toc');

            if (localStorage.getItem('tocOpen') === 'true') {
                tocPanel.classList.add('open');
                localStorage.removeItem('tocOpen');
            }

            tocButton.addEventListener('click', function(e) {
                e.stopPropagation();
                tocPanel.classList.toggle('open');
            });

            function closeTocPanel() {
                tocPanel.classList.remove('open');
            }

            closeToc.addEventListener('click', function(e) {
                e.stopPropagation();
                closeTocPanel();
            });

            document.addEventListener('click', function(e) {
                if (!tocPanel.contains(e.target) && e.target !== tocButton) {
                    closeTocPanel();
                }
            });

            tocPanel.addEventListener('click', function(e) {
                e.stopPropagation();
            });

            document.querySelectorAll('#toc-panel a').forEach(link => {
                link.addEventListener('click', function(e) {
                    if (!this.getAttribute('href').startsWith('#')) {
                        localStorage.setItem('tocOpen', 'true');
                    }
                    if (this.hash) {
                        e.preventDefault();
                        const target = document.getElementById(this.hash.substring(1));
                        if (target) {
                            window.scrollTo({
                                top: target.offsetTop - 70,
                                behavior: 'smooth'
                            });
                            history.pushState(null, null, this.hash);
                            if (window.innerWidth < 768) {
                                closeTocPanel();
                            }
                        }
                    }
                });
            });

            if (window.location.hash) {
                const target = document.getElementById(window.location.hash.substring(1));
                if (target) {
                    setTimeout(() => {
                        window.scrollTo({
                            top: target.offsetTop - 70,
                            behavior: 'smooth'
                        });
                    }, 100);
                }
            }
        });
    </script>
</body>
</html>
