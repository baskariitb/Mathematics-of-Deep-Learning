<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Expressivity Over Function Spaces</title>
    <link rel="stylesheet" href="styles.css">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML ">
    </script>
    <style>
        body, p, li, div {
        color: black !important;
        line-height: 1.6;
        }
        ol.latex-enumerate, ol.latex-enumerate li {
            line-height: 1.6 !important;
            margin-top: 0.5em;
            margin-bottom: 0.5em;
        }
        /* Updated TOC styles */
        #toc-panel {
            font-size: 0.9em;
        }
        .toc-chapter, 
        .toc-section, 
        .toc-subsection {
            color: black !important;
        }
        .toc-chapter a,
        .toc-section a,
        .toc-subsection a {
            color: black !important;
            text-decoration: none;
        }
        .toc-chapter.current {
            background-color: rgba(26, 115, 232, 0.05);
        }
        #toc-button {
            position: fixed;
            top: 10px;
            left: 10px;
            background: #1a73e8;
            color: white;
            padding: 10px 15px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            z-index: 1000;
            box-shadow: 0 2px 5px rgba(0,0,0,0.2);
        }
        #toc-panel {
            position: fixed;
            top: 0;
            left: -350px;
            width: 300px;
            height: 100%;
            background: white;
            box-shadow: 2px 0 10px rgba(0,0,0,0.1);
            z-index: 999;
            padding: 20px;
            overflow-y: auto;
            transition: left 0.3s ease-out;
            will-change: transform;
        }
        #toc-panel.open {
            left: 0;
        }
        #close-toc {
            position: absolute;
            top: 10px;
            right: 10px;
            background: none;
            border: none;
            font-size: 20px;
            cursor: pointer;
        }
        #content {
            padding: 20px;
            padding-top: 60px;
            max-width: 100%;
            overflow-x: hidden;
            word-wrap: break-word;
        }
        .toc-chapter {
            margin-bottom: 10px;
            border-left: 3px solid #94714B;
            padding-left: 10px;
        }
        .toc-chapter.current {
            background-color: #F0EAE4;
        }
        .chapter-link {
            font-weight: bold;
            color: #1a73e8;
            text-decoration: none;
            display: block;
            padding: 5px 0;
        }
        .toc-sections {
            margin-left: 15px;
            display: none;
        }
        .toc-sections.expanded {
            display: block;
        }
        .toc-section {
           font-weight: bold;
            padding: 3px 0;
            margin-left: 10px;
        }
        .toc-subsection {
            padding: 1.5px 0;
            margin-left: 25px;
            font-size: 0.95em;
        }
        .toc-subsubsection {
            padding: 3px 0;
            margin-left: 40px;
            font-size: 0.9em;
            color: #555;
        }
        html {
            scroll-behavior: smooth;
        }
        :target {
            scroll-margin-top: 80px;
            animation: highlight 1.5s ease;
        }
        @keyframes highlight {
            0% { background-color: rgba(26, 115, 232, 0.1); }
            100% { background-color: transparent; }
        }
        @media (max-width: 768px) {
            #toc-panel {
                width: 85%;
                left: -90%;
            }
            #toc-panel.open {
                left: 0;
                box-shadow: 4px 0 15px rgba(0,0,0,0.2);
            }
            #content {
                transition: transform 0.3s ease-out;
            }
            #toc-panel.open ~ #content {
                transform: translateX(85%);
            }
        }
        .note {
  background-color: color-mix(in srgb, var(--primary) 7%, white);
border: 5px solid var(--primary);
  padding: 5px;
  margin: 10px;
}
.problem {
  background-color: color-mix(in srgb, var(--primary) 18%, white);
border: 1px solid var(--primary);
  padding: 10px;
  margin: 10px;
}
.example {
  background-color: var(--environmentBackground);
border-top: 7px solid var(--primary);
border-bottom: 7px solid var(--primary);
border-left: none;
border-right: none;
  padding: 10px;
  margin: 10px;
}
.remark {
  background-color: var(--environmentBackground);
border-top: 7px solid var(--primary);
border-bottom: 7px solid var(--primary);
border-left: none;
border-right: none;
  padding: 20px;
  margin: 10px;
}
.definition {
  background-color: color-mix(in srgb, var(--primary) 10%, white);
border: 3px double var(--primary);
  padding: 10px;
  margin: 10px;
}
.lemma {
  background-color: color-mix(in srgb, var(--primary) 10%, white);
border: 3px solid var(--primary);
  padding: 10px;
  margin: 10px;
  box-shadow: 0px 0px 10px rgba(0,0,0,0.1);
}
.theorem {
  background-color: color-mix(in srgb, var(--primary) 10%, white);
border: 3px solid var(--primary);
  padding: 10px;
  margin: 10px;
  box-shadow: 0px 0px 10px rgba(0,0,0,0.1);
}
.proofs {
  background-color: var(--environmentBackground);
border: 5px solid var(--primary);
  padding: 10px;
  margin: 10px;
}
.project {
  background-color: var(--environmentBackground);
border: 3px solid var(--primary);
  padding: 10px;
  margin: 10px;
}
.hint {
  background-color: color-mix(in srgb, var(--primary) 10%, white);
border: 3px solid var(--primary);
  padding: 10px;
  margin: 10px;
}
.code {
  background-color: color-mix(in srgb, var(--primary) 13%, white);
border: 3px solid var(--primary);
  padding: 10px;
  margin: 10px;
}
.algorithm {
  background-color: color-mix(in srgb, var(--primary) 5%, white);
border: 3px solid var(--primary);
  padding: 10px;
  margin: 10px;
}
    </style>
</head>


<body>
    <button id="toc-button">☰ Table of Contents</button>
    <div id="toc-panel">
        <button id="close-toc">×</button>
       
       <!-- Small Main Page Button -->
<div style="text-align:right; margin-bottom: 10px;">
  <a href="index.html" 
     style="
       background: var(--toc-primary, #94714B);
       color: white;
       padding: 4px 10px;
       border-radius: 16px;
       font-size: 0.75em;
       font-weight: 900;
       text-decoration: none;
       display: inline-block;
       box-shadow: 0 2px 6px rgba(0,0,0,0.12);
     ">
    Main Page
  </a>
</div>
       
        <div class="full-toc">

                <div class="toc-chapter ">
            <a href="HTMLIntroductionNeurons.html" class="chapter-link">
                CH 1: Introduction to Neurons
            </a>
             <div class="toc-sections ">

                <div class="toc-item ">
                    <a href="HTMLIntroductionNeurons.html#introduction.neurons.ch">
                        Introduction to Neurons
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLIntroductionNeurons.html#motivating.example.sec">
                        Motivating Example
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLIntroductionNeurons.html#water-source-sink-control-problem">
                        Water Source-Sink Control Problem
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLIntroductionNeurons.html#electric.circuit.ssec">
                        Electric Circuit
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLIntroductionNeurons.html#linear.regression.subs">
                        Linear Regression
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLIntroductionNeurons.html#artificial.neurons.sec">
                        Artificial Neurons
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLIntroductionNeurons.html#mathematical.formulation.subs">
                        Mathematical Formulation
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLIntroductionNeurons.html#comparison-with-biological-neurons">
                        Comparison with Biological Neurons
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLIntroductionNeurons.html#learning.model.subs">
                        Supervised Learning: An Overview
                    </a>
                </div>
</div></div>

        <div class="toc-chapter ">
            <a href="HTMLPerceptrons.html" class="chapter-link">
                CH 2: Perceptrons
            </a>
            <div class="toc-sections ">

                <div class="toc-item ">
                    <a href="HTMLPerceptrons.html#perceptrons.ch">
                        Perceptrons
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLPerceptrons.html#perceptrons.sec">
                        Neuron Model
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLPerceptrons.html#boolean-functions">
                        Boolean Functions
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLPerceptrons.html#illustrative-datasets">
                        Illustrative Datasets
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLPerceptrons.html#separability-and-algorithm">
                        Separability and Algorithm
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLPerceptrons.html#linear-separability">
                        Linear Separability
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLPerceptrons.html#training-algorithm">
                        Training Algorithm
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLPerceptrons.html#perceptron.convergence.theorem.subs">
                        Perceptron Convergence Theorem
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLPerceptrons.html#beyond-linear-separability">
                        Beyond Linear Separability
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLPerceptrons.html#feature-space-mapping">
                        Feature Space Mapping
                    </a>
                </div>
</div></div>

        <div class="toc-chapter ">
            <a href="HTMLSupportVectorMachine.html" class="chapter-link">
                CH 3: Support Vector Machine
            </a>
            <div class="toc-sections ">

                <div class="toc-item ">
                    <a href="HTMLSupportVectorMachine.html#support-vector-machine">
                        Support Vector Machine
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLSupportVectorMachine.html#linearly-separable-dataset">
                        Linearly Separable Dataset
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLSupportVectorMachine.html#hard-and-soft-margins">
                        Hard and Soft Margins
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLSupportVectorMachine.html#loss-function-form">
                        Loss-function form
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLSupportVectorMachine.html#subgradient-descent-algorithm">
                        Subgradient Descent Algorithm
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLSupportVectorMachine.html#dual-optimazation-problem">
                        Dual Optimazation Problem
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLSupportVectorMachine.html#nonlinearly-separable-datasets">
                        Nonlinearly Separable Datasets
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLSupportVectorMachine.html#kernel-method-implicit-feature-mapping">
                        Kernel Method: Implicit Feature Mapping
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLSupportVectorMachine.html#kernel-trick">
                        Kernel Trick
                    </a>
                </div>
</div></div>

        <div class="toc-chapter ">
            <a href="HTMLMultilayerPerceptrons.html" class="chapter-link">
                CH 4: Multilayer Perceptrons
            </a>
            <div class="toc-sections ">

                <div class="toc-item ">
                    <a href="HTMLMultilayerPerceptrons.html#multilayer.perceptrons.ch">
                        Multilayer Perceptrons
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLMultilayerPerceptrons.html#fully-connected-feedforward-networks">
                        Fully Connected Feedforward Networks
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#definition-and-network-dimensions">
                        Definition and Network Dimensions
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#shallow-networks">
                        Shallow Networks
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#batch-forward-propagation">
                        Batch Forward Propagation
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#batch-propagation">
                        Batch Propagation
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLMultilayerPerceptrons.html#activation-functions">
                        Activation Functions
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#step-functions">
                        Step Functions
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#heaviside-function">
                        Heaviside function
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#bipolar-function">
                        Bipolar Function
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#sigmoid-functions-logistic-regression">
                        Sigmoid Functions: Logistic Regression
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#hyperbolic-tangent-function">
                        Hyperbolic Tangent Function
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#bumped-type-functions">
                        Bumped-type Functions
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#hockey-stick-functions">
                        Hockey-stick Functions
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#leaky-rectified-linear-unit">
                        Leaky Rectified Linear Unit
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#sigmoid-linear-units">
                        Sigmoid Linear Units
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#softplus-function">
                        Softplus Function
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLMultilayerPerceptrons.html#multi-class-classification">
                        Multi-class Classification
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLMultilayerPerceptrons.html#softmax-function">
                        Softmax Function
                    </a>
                </div>
</div></div>

        <div class="toc-chapter ">
            <a href="HTMLTrainingNeuralNetwork.html" class="chapter-link">
                CH 5: Training Neural Networks
            </a>
            <div class="toc-sections ">

                <div class="toc-item ">
                    <a href="HTMLTrainingNeuralNetwork.html#learning.mechanism.ch">
                        Training Neural Networks
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLTrainingNeuralNetwork.html#cost.functions.sec">
                        Cost Functions
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#mean-square-error">
                        Mean Square Error
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#cross-entropy">
                        Cross-Entropy
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#kullback-leibler-divergence">
                        Kullback-Leibler divergence
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLTrainingNeuralNetwork.html#backpropagation.sec">
                        Backpropagation Algorithm
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#chain-rule-and-gradient-computation">
                        Chain Rule and Gradient Computation
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#gradient-for-a-neuron">
                        Gradient for a Neuron
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#shallow-network">
                        Shallow Network
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#deep-network">
                        Deep Network
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLTrainingNeuralNetwork.html#weight-updates-using-gradient-descent">
                        Weight Updates Using Gradient Descent
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#batch-gradient-descent-method">
                        Batch Gradient Descent Method
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#stochastic-gradient-descent-sgd">
                        Stochastic Gradient Descent (SGD)
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#mini-batch-gradient-descent">
                        Mini-batch Gradient Descent
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#momentum-method">
                        Momentum Method
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#adaptive-step-size-methods">
                        Adaptive Step Size Methods
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#adagrad">
                        AdaGrad.
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#root-mean-square-propagation-rmsprop">
                        Root Mean Square Propagation (RMSProp)
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#adaptive-moments-adam">
                        Adaptive Moments (Adam).
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLTrainingNeuralNetwork.html#regularization-and-generalization">
                        Regularization and Generalization
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#underfitting-and-overfitting">
                        Underfitting and Overfitting
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#parameter-based-regularization">
                        Parameter-based Regularization
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#process-based-regularization">
                        Process-based Regularization
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#early-stopping">
                        Early Stopping
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#dropout">
                        Dropout
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLTrainingNeuralNetwork.html#generalization">
                        Generalization
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLTrainingNeuralNetwork.html#bias-variance-tradeoff">
                        Bias-Variance Tradeoff
                    </a>
                </div>
</div></div>

        <div class="toc-chapter current">
            <a href="HTMLExpressivityoverFunctionSpaces.html" class="chapter-link">
                CH 6: Expressivity Over Function Spaces
            </a>
            <div class="toc-sections expanded">

                <div class="toc-item ">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#expressivity.over.function.spaces.ch">
                        Expressivity over Function Spaces
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#universal.approximation.sec">
                        Function Approximations
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#continuous-function-approximations">
                        Continuous Function Approximations
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#feature-mapping-perspective">
                        Feature Mapping Perspective
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#connection-to-kernel-machines">
                        Connection to kernel machines
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#learning-dynamics-and-the-neural-tangent-kernel">
                        Learning dynamics and the Neural Tangent Kernel
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#pinns.sec">
                        Physics Informed Neural Networks for ODEs
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#trial-solution">
                        Trial Solution
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#residual-and-training-loss">
                        Residual and Training Loss
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#training-algorithm">
                        Training Algorithm
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLExpressivityoverFunctionSpaces.html#research-perspectives">
                        Research Perspectives
                    </a>
                </div>
</div></div>

        <div class="toc-chapter ">
            <a href="HTMLSpecialArchitectures.html" class="chapter-link">
                CH 7: Special Architectures
            </a>
            <div class="toc-sections ">

                <div class="toc-item ">
                    <a href="HTMLSpecialArchitectures.html#advanced.architectures.ch">
                        Special Architectures
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLSpecialArchitectures.html#convolution.neural.network.sec">
                        Convolution Neural Network
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLSpecialArchitectures.html#networks-of-one-dimensional-signals">
                        Networks of One-Dimensional Signals
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLSpecialArchitectures.html#one-dimensional-convolutional-layer">
                        One-Dimensional Convolutional Layer
                    </a>
                </div>

                <div class="toc-item toc-subsection">
                    <a href="HTMLSpecialArchitectures.html#two-dimensional-convolutional-layer">
                        Two-Dimensional Convolutional Layer
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLSpecialArchitectures.html#convolution-operator">
                        Convolution Operator
                    </a>
                </div>

                <div class="toc-item toc-subsubsection">
                    <a href="HTMLSpecialArchitectures.html#2d-convolution-layer">
                        2D Convolution Layer
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLSpecialArchitectures.html#recurrent-neural-networks">
                        Recurrent Neural Networks
                    </a>
                </div>

                <div class="toc-item toc-section">
                    <a href="HTMLSpecialArchitectures.html#transformers">
                        Transformers
                    </a>
                </div>
</div></div>
</div>
    </div>
    <div id="content">
        <p>
<h1 id="expressivity.over.function.spaces.ch">Expressivity over Function Spaces</h1></p><p>Having understood the fundamental theory of deep neural networks from the previous chapters, we now ask the most important question of </p><p>	<i>Why should neural networks work?</i></p><p>This is a difficult question to answer from a practical perspective. We can certainly point to abundant empirical evidence from real-world applications to believe that deep learning models are successfully used in a wide range of domains such as image recognition, natural language processing, and so on.  However, to provide a more concrete and theoretical answer to the above question, we turn to several remarkable mathematical results concerning the function approximation capacity of neural networks.  In this chapter, we review a few of these results.</p><p>A neural network can be effectively used in two major ways.  One is for <b>pattern recognition</b>, which essentially corresponds to a <b>classification problem</b> as illustrated often in our previous chapters. Another important application of deep learning is to use it as a <b>function approximation</b> tool, in particular in regression, where the goal is to approximate an underlying functional relationship between inputs and outputs.  In fact, classification problems can also be viewed as function approximation as we are effectively approximating a function that maps the input vectors to discrete output labels.</p><p>In recent years, deep learning has also been used to construct approximate solutions to differential equations by incorporating the underlying physics into the cost function through residual errors. Such networks are known as <b>physics informed neural networks</b> (PINN).   PINNs provide a natural conceptual bridge from approximation theory to scientific machine learning.</p><p>In this chapter, we review few basic results that demonstrate the approximation capacity of neural networks.
In <a href="HTMLExpressivityoverFunctionSpaces.html#universal.approximation.sec" class="internal-link">Section  &laquo;Click Here&raquo;</a>, we shall see that even a simple feedforward network with a single hidden layer can approximate, to any desired accuracy, any continuous function defined on a compact domain.  This remarkable property is known as the <b>universal approximation property</b>. </p><p><h2 id="universal.approximation.sec">Function Approximations</h2></p><p>Approximating a nonlinear function is challenging because different types of nonlinearities require different function classes for effective approximation.  The ultimate requirement in approximation theory is to identify a suitable function class within which we can find an approximation to any given nonlinear function to a desired accuracy.  Such a property is referred to as <b>universal approximation</b>.  Deep neural networks are known to be good universal approximators, in the sense that, by suitably choosing their architecture, one can achieve any desired accuracy for a wide class of target functions.</p><p>Let <span class="math">\(\mathcal{X}\)</span> denote the input space and <span class="math">\(\mathcal{Y}\)</span> the target space.  </p><p>Define a function class
</p><div class="math">\[
\mathbb{F}(\mathcal{X},\mathcal{Y}):= \big\{ f:\mathcal{X} \rightarrow \mathcal{Y}~\big|~ f ~ \text{possesses certain properties}\big\}.
\]</div><p>
A <b>hypothesis class</b> is a subclass <span class="math">\(\mathcal{H}\subseteq \mathbb{F}\)</span> from which we seek an approximation to a given target function <span class="math">\(f^*\in \mathbb{F}\)</span>.</p><p>In what follows, we consider the function class <span class="math">\(\mathbb{F}\)</span> as a linear space equipped with a norm, thereby making it a normed linear space.  In this case, we call the normed linear space a <b>function space</b>, especially if it is a Banach space or a Hilbert space.</p><p><div class="example">
<div class="heading-container">
<b class="heading">Example:</b>
</div>
</p><p><b>Continuous Functions:</b></p><p>
<li>Let <span class="math">\(\mathcal{X}\subset \mathbb{R}^{n}\)</span> be compact and <span class="math">\(\mathcal{Y}\subseteq \mathbb{R}^m\)</span>. Then, 
</p><div class="math">\[
\mathbb{F}(\mathcal{X},\mathcal{Y}):= \big\{ f:\mathcal{X} \rightarrow \mathcal{Y}~\big|~ f ~ \text{is continuous}\big\}
\]</div><p>
is a function space with the <b>uniform norm</b> (or <b>supremum norm</b>) defined as
</p><div class="math">\[
\|f\|_{\infty,\mathcal{X}} := \sup\{\|f(\boldsymbol{x})\|~|~\boldsymbol{x}\in \mathcal{X} \},
\]</div><p>
where <span class="math">\(\|\cdot\|\)</span> on the right hand side denotes a vector norm on <span class="math">\(\mathbb{R}^m\)</span>, which is often taken as the Euclidean norm. One may also take the <span class="math">\(\ell^1\)</span>-norm or the maximum norm.</p><p>We use the notation <span class="math">\(\mathbb{C}(\mathcal{X}, \mathcal{Y})\)</span> to denote the space of continuous functions. In particular, if <span class="math">\(\mathcal{Y}=\mathbb{R}\)</span>, then we use the notation <span class="math">\(\mathbb{C}(\mathcal{X})\)</span>.</p><p>Since <span class="math">\(\mathcal{X}\)</span> is compact, <span class="math">\((\mathbb{C}(\mathcal{X}, \mathcal{Y}), \|\cdot\|_{\infty,\mathcal{X}})\)</span> is a Banach space.
</p><p><b>Continuously Differentiable Functions:</b></p><p>
</li><li>Let <span class="math">\(\mathcal{X}\subset \mathbb{R}^{n}\)</span> be compact, <span class="math">\(\mathcal{Y}\subseteq \mathbb{R}^m\)</span>, and let <span class="math">\(k\in \mathbb{N}\)</span>. Then, 
</p><div class="math">\[
\mathbb{F}(\mathcal{X},\mathcal{Y}):= \big\{ f:\mathcal{X} \rightarrow \mathcal{Y}~\big|~ f ~ \text{is } k\text{-times continuously differentiable}\big\}
\]</div><p>
is a function space with the norm defined as
</p><div class="math">\[
\|f\|_{\infty,k,\mathcal{X}} := \max_{|\boldsymbol{\alpha}|\le k}~\big(\sup\{\|\partial^{\boldsymbol{\alpha}} f(\boldsymbol{x})\|~|~\boldsymbol{x}\in \mathcal{X}, \boldsymbol{\alpha}=(\alpha_1,\ldots,\alpha_n)\in \mathbb{N}_0^n \} \big),
\]</div><p>
where <span class="math">\(\|\cdot\|\)</span> on the right hand side is a vector norm on <span class="math">\(\mathbb{R}^m\)</span>, <span class="math">\(|\boldsymbol{\alpha}|=\alpha_1+\ldots+\alpha_n,\)</span> and <span class="math">\(\mathbb{N}_0^n = (\mathbb{N}\cup \{0\})^n\)</span> is the multi-index set.</p><p>We use the notation <span class="math">\(\mathbb{C}^k(\mathcal{X}, \mathcal{Y})\)</span> to denote the space of <span class="math">\(k\)</span>-times continuously differentiable functions and is a Banach space since <span class="math">\(\mathcal{X}\)</span> is compact.</p><p>We use the notation <span class="math">\(\mathbb{C}^k(\mathcal{X})\)</span>, if <span class="math">\(\mathcal{Y}=\mathbb{R}\)</span>.
</p><p><b><span class="math">\(L^p\)</span> Functions:</b>

</li><li>Let <span class="math">\((\mathcal{X}, \Sigma, \mu)\)</span> be a measure space and <span class="math">\(\mathcal{Y} = \mathbb{R}^m\)</span>. Then, 
</p><div class="math">\[
\mathbb{F}(\mathcal{X}, \mathcal{Y}) := 
\Big\{ f:\mathcal{X} \rightarrow \mathcal{Y}~\big|~ 
\int_{\mathcal{X}} \|f(\boldsymbol{x})\|^p\, d\mu(\boldsymbol{x}) < \infty \Big\}, 
\quad 1 \le p < \infty,
\]</div><p>
where <span class="math">\(\|\cdot\|\)</span> denotes a vector norm on <span class="math">\(\mathbb{R}^m\)</span>, often taken as the Euclidean norm.  </p><p>The corresponding norm on this space is defined as
</p><div class="math">\[
\|f\|_{L^p} := 
\Bigg( \int_{\mathcal{X}} \|f(\boldsymbol{x})\|^p\, d\mu(\boldsymbol{x}) \Bigg)^{\!1/p}.
\]</div><p>
For <span class="math">\(p = \infty\)</span>, we define
</p><div class="math">\[
\|f\|_{L^\infty} := 
\operatorname*{ess\,sup}_{\boldsymbol{x} \in \mathcal{X}} \|f(\boldsymbol{x})\|,
\]</div><p>
where <span class="math">\(\operatorname*{ess\,sup}\)</span> denotes the essential supremum with respect to the measure <span class="math">\(\mu\)</span>.</p><p>The space of such functions is denoted by <span class="math">\(\mathbb{L}^p(\mathcal{X}, \mathcal{Y})\)</span>, 
and is called the <b><span class="math">\(L^p\)</span>-space</b>.   The <span class="math">\(L^p\)</span> spaces are Banach spaces, and in the special case <span class="math">\(p=2\)</span>, 
<span class="math">\(\mathbb{L}^2(\mathcal{X}, \mathcal{Y})\)</span> is a Hilbert space with the inner product
</p><div class="math">\[
\langle f, g \rangle_{L^2} := 
\int_{\mathcal{X}} \langle f(\boldsymbol{x}), g(\boldsymbol{x}) \rangle\, d\mu(\boldsymbol{x}),
\]</div><p>
where <span class="math">\(\langle \cdot, \cdot \rangle\)</span> is the Euclidean inner product on <span class="math">\(\mathbb{R}^m\)</span>.</p><p>It is common to use the notation <span class="math">\(\mathbb{L}^p(\mathcal{X})\)</span> if <span class="math">\( \mathcal{Y}=\mathbb{R}\)</span>.
</p><p><b>Sobolev Functions:</b>

</li><li>Let <span class="math">\(\mathcal{X}\subset \mathbb{R}^{n}\)</span> be an open set and <span class="math">\(\mathcal{Y} = \mathbb{R}^m\)</span>.  
For <span class="math">\(k \in \mathbb{N}\)</span> and <span class="math">\(1 \le p \le \infty\)</span>, we define the function class
</p><div class="math">\[
\mathbb{F}(\mathcal{X}, \mathcal{Y}) :=
\Big\{ f:\mathcal{X} \rightarrow \mathcal{Y}~\big|~
\partial^{\boldsymbol{\alpha}} f \in L^p(\mathcal{X}, \mathcal{Y}) 
\text{ for all multi-indices } \boldsymbol{\alpha} 
\text{ with } |\boldsymbol{\alpha}| \le k \Big\}.
\]</div><p>The corresponding norm is defined as
</p><div class="math">\[
\|f\|_{W^{k,p}(\mathcal{X})}
:=
\Bigg( 
\sum_{|\boldsymbol{\alpha}| \le k} 
\|\partial^{\boldsymbol{\alpha}} f\|_{L^p(\mathcal{X})}^p
\Bigg)^{\!1/p},
\qquad 1 \le p < \infty,
\]</div><p>
and for <span class="math">\(p = \infty\)</span>,
</p><div class="math">\[
\|f\|_{W^{k,\infty}(\mathcal{X})}
:=
\max_{|\boldsymbol{\alpha}| \le k} 
\|\partial^{\boldsymbol{\alpha}} f\|_{L^\infty(\mathcal{X})}.
\]</div><p>The space of such functions is denoted by 
</p><div class="math">\[
\mathbb{W}^{k,p}(\mathcal{X}, \mathcal{Y}),
\]</div><p>
and is called the <b>Sobolev space</b>.  </p><p>The Sobolev spaces <span class="math">\(\mathbb{W}^{k,p}(\mathcal{X}, \mathcal{Y})\)</span> are <b>Banach spaces</b>.
For the special case <span class="math">\(p=2,\)</span> we use the notation <span class="math">\(\mathbb{H}^k(\mathcal{X},\mathcal{Y}) := \mathbb{W}^{k,2}(\mathcal{X},\mathcal{Y})\)</span>, which is a Hilbert space with the inner product
</p><div class="math">\[
\langle f, g \rangle_{\mathbb{H}^k}
:=
\sum_{|\boldsymbol{\alpha}| \le k}
\langle \partial^{\boldsymbol{\alpha}} f, \partial^{\boldsymbol{\alpha}} g \rangle_{L^2(\mathcal{X})}.
\]</div><p><b>Bounded Variation (BV) Functions:</b>

</li><li>Let <span class="math">\(\mathcal{X} \subset \mathbb{R}^n\)</span> be an open set and <span class="math">\(\mathcal{Y} = \mathbb{R}\)</span>.  
A function <span class="math">\(f:\mathcal{X} \to \mathcal{Y}\)</span> is said to be of <b>bounded variation</b> if <span class="math">\(f \in L^1(\mathcal{X})\)</span> and its distributional derivatives are finite Radon measures.  </p><p>The space of functions of bounded variation is denoted by
</p><div class="math">\[
\mathbb{BV}(\mathcal{X}) := \Big\{ f \in L^1(\mathcal{X}) \;\big|\; 
\|f\|_{\mathbb{BV}} < \infty \Big\},
\]</div><p>
where the <b>BV-norm</b> is defined as
</p><div class="math">\[
\|f\|_{\mathbb{BV}} := \|f\|_{L^1(\mathcal{X})} + |Df|(\mathcal{X}),
\]</div><p>
and <span class="math">\(|Df|(\mathcal{X})\)</span> denotes the total variation of the derivative measure <span class="math">\(Df\)</span> over <span class="math">\(\mathcal{X}\)</span>.</p><p>The space <span class="math">\(\mathbb{BV}(\mathcal{X})\)</span> is a <b>Banach space</b> with respect to the norm <span class="math">\(\|\cdot\|_{\mathbb{BV}}\)</span>.
</p><p>
</div></p><p>Often, solutions of mathematical problems are functions, and it is necessary to look for the solution in an appropriate function space. For many problems, we may know that a solution exists in a given function space, but its explicit form is either unknown or too complicated to obtain.  In such situations, we look for an approximation to the solution in the function space.  However, the full function space is often too large that obtaining an approximation within this space becomes impossible, and we have to restrict to a more tractable subset, a hypothesis set. To ensure that this hypothesis set is sufficient to approximate any solution to a desired accuracy, it must be <b>dense</b> in the function space.</p><p><div class="definition">
<div class="heading-container">
<b class="heading">Definition:</b>
</div>
[<b>Dense Subset</b>]</p><p>A subset <span class="math">\(\mathcal{H}\)</span> of a normed linear space <span class="math">\(\mathbb{F}\)</span> is said to be <b>dense</b> in <span class="math">\(\mathbb{F}\)</span> if for every <span class="math">\(f\in \mathbb{F}\)</span>, and every <span class="math">\(\varepsilon>0\)</span>, there exists <span class="math">\(h\in \mathcal{H}\)</span> such that
</p><div class="math">\[
\|f - h\| < \varepsilon.
\]</div><p>
</div></p><p><div class="problem">
<div class="heading-container">
<b class="heading">Problem:</b>
</div>
Let <span class="math">\(\mathcal{H}\)</span> be a dense subset of the normed linear space <span class="math">\(\mathbb{F}\)</span>.  Then show that for every <span class="math">\(f\in \mathbb{F}\)</span>, there exists a sequence <span class="math">\(\{h_n\}\)</span> in <span class="math">\(\mathcal{H}\)</span> such that <span class="math">\(h_n\rightarrow f\)</span> as <span class="math">\(n\rightarrow \infty\)</span>.
</div></p><p>We now introduce a notation for the set of all ANNs.</p><p><div class="remark">
<div class="heading-container">
<b class="heading">Remark:</b>
</div>
[<b>Family of Feedforward Neural Networks</b>]</p><p>Given <span class="math">\(L, \hat{n}, n_0, n_L\in \mathbb{N}\)</span>, and an activation function <span class="math">\(\mathscr{A}:\mathbb{R}\rightarrow \mathbb{R}\)</span>, the set of all multilayer perceptrons of depth <span class="math">\(L\)</span> and width <span class="math">\(\hat{n}\)</span> is denoted by
</p><div class="math">\[
\mathscr{N}_{L,\hat{n}}(\mathscr{A}; n_0, n_L) = \Big\{
						{\mathscr{F}_{L,\hat{n}}}: \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_L}~\Big|~  \mathscr{A}^{(l)} = \mathscr{A},  l=1,2,\ldots, L-1, \mathscr{A}^{(L)}(x) = x
						\Big\}.
\]</div><p>
Further, we use the notation
</p><div class="math">\[
\mathscr{N}_{L}(\mathscr{A}; n_0, n_L) = \displaystyle{\bigcup_{\hat{n}\in \mathbb{N}}} \mathscr{N}_{L,\hat{n}}(\mathscr{A}; n_0, n_L)
\]</div><p>
to denote the set of all multilayer perceptrons of depth <span class="math">\(L\)</span> and arbitrary but finite hidden widths.</p><p>
</div></p><p><h3 id="continuous-function-approximations">Continuous Function Approximations</h3></p><p>In this subsection, we study the expressivity feed forward neural networks as approximations to continuous function on a compact set. Let us assume that the input set <span class="math">\(\mathcal{X}\subset \mathbb{R}^{n_0}\)</span> is compact.</p><p>We call a set <span class="math">\(\mathcal{H}\)</span> a 
<b>universal approximator</b> of <span class="math">\(\mathbb{C}(\mathbb{R}^n)\)</span> if, for every compact set 
<span class="math">\(\mathcal{X} \subset \mathbb{R}^n\)</span>, the restriction 
<span class="math">\(\mathcal{H}|_{\mathcal{X}} = \{h|_{\mathcal{X}} : h \in \mathcal{H}\}\)</span> 
is dense in <span class="math">\(\mathbb{C}(\mathcal{X})\)</span> with respect to the uniform norm. 
In other words, for every <span class="math">\(f \in \mathbb{C}(\mathcal{X})\)</span> and every <span class="math">\(\varepsilon > 0\)</span>, 
there exists <span class="math">\(h \in \mathcal{H}\)</span> such that 
</p><div class="math">\[
\sup_{x \in \mathcal{X}} |f(x) - h(x)| < \varepsilon.
\]</div><p>The following are some classical examples of universal approximators.</p><p><div class="example">
<div class="heading-container">
<b class="heading">Example:</b>
</div>
[<b>Multivariate Polynomial Function</b>]</p><p>Recall that a <b>polynomial function</b> <span class="math">\(p : \mathbb{R}^n \to \mathbb{R}\)</span> is of the form
</p><div class="math">\[
f(\boldsymbol{x}) = \sum_{\boldsymbol{\alpha} \in \mathbb{N}_0^n} c_{\boldsymbol{\alpha}}\, \boldsymbol{x}^{\boldsymbol{\alpha}},~\boldsymbol{x}=(x_1, x_2, \ldots, x_n)\in \mathbb{R}^n,
\]</div><p>
where only finitely many coefficients <span class="math">\( c_{\boldsymbol{\alpha}} \in \mathbb{R} \)</span> are nonzero, for each multi-index <span class="math">\(\boldsymbol{\alpha}=(\alpha_1, \alpha_2,\ldots, \alpha_n)\in \mathbb{N}_0^n\)</span> the monomial is defined as
</p><div class="math">\[
\boldsymbol{x}^{\boldsymbol{\alpha}} = \prod_{k=1}^n x_k^{\alpha_k}
\]</div><p>
with degree of the monomial being <span class="math">\( |\boldsymbol{\alpha}| = \alpha_1 + \alpha_2 + \cdots + \alpha_n.\)</span> The <b>degree</b> of the polynomial <span class="math">\( p \)</span> is the largest <span class="math">\( |\boldsymbol{\alpha|} \)</span> such that <span class="math">\( c_\alpha \neq 0 \)</span>.</p><p>Consider the function space <span class="math">\(\mathbb{C}(\mathbb{R}^n)\)</span>. By Stone-Weierstrass' theorem, the space of all polynomials <span class="math">\(\mathcal{P}(\mathcal{X})\)</span> on any compact set <span class="math">\(\mathcal{X}\subset \mathbb{R}^n\)</span> is a dense subspace of  <span class="math">\(\mathbb{C}(\mathcal{X})\)</span>. Hence 
<span class="math">\(\mathcal{P}(\mathbb{R}^n)\)</span> is an universal approximator of <span class="math">\(\mathbb{C}(\mathbb{R}^n)\)</span>.
</div></p><p>The following lemma gives another dense subset of the space of continuous functions.  We restrict our discussion to <span class="math">\(n=1\)</span> and <span class="math">\(m=1\)</span> and omit the discussions on multi-dimensions.</p><p><div class="lemma" id="denseness.simple.functions.lemm">
<div class="heading-container">
<b class="heading">Lemma:</b>
</div>
[<b>Expressivity of Simple Functions</b>]
</p><p>For every <span class="math">\(f \in \mathbb{C}([a,b])\)</span> and every <span class="math">\(\varepsilon > 0\)</span>, there exists a simple function <span class="math">\(s_\varepsilon: [a,b]\rightarrow \mathbb{R}\)</span> such that
</p><div class="math">\[
\|f - s_\varepsilon\|_{\infty,[a,b]} < \varepsilon.
\]</div><p>
</div></p><p><div class="proofs">
<div class="heading-container">
<b class="heading">Proof:</b>
</div>
Since <span class="math">\(f\)</span> is continuous on the compact interval <span class="math">\([a,b]\)</span>, it is uniformly continuous. Hence, by definition, for every <span class="math">\(\varepsilon>0\)</span>, we can find a <span class="math">\(\delta>0\)</span> (depends only on <span class="math">\(\varepsilon\)</span>) such that, for any <span class="math">\(x,y\in [a,b]\)</span> with <span class="math">\(|x-y|<\delta\)</span>,  we have <span class="math">\(|f(x) - f(y)|<\varepsilon.\)</span></p><p>Therefore, choosing a sufficiently large <span class="math">\(N>0\)</span> such that <span class="math">\((b-a)/N<\delta\)</span>, we can have the uniform partition 
</p><div class="math">\[
a = x_0 < x_1 < \dots < x_N = b,
\]</div><p>
on which we have
</p><div class="math">\[
|f(x) - f(y)| < \varepsilon, \quad \text{for all } x,y \in [x_k, x_{k+1}],~k=0,\dots,N-1.
\]</div><p>
Define a step function <span class="math">\(s_\varepsilon:[a,b]\to\mathbb{R}\)</span> by
</p><div class="math">\[
\begin{array}{cc}
s_\varepsilon(x) = f(x_k), &amp;\text{for } x \in [x_k, x_{k+1}), \quad k = 0,1,\dots,N-1,\\
s_\varepsilon(1) = f(x_{N-1}).&amp;
\end{array}
\]</div><p>
Since, for any <span class="math">\(x\in [a,b)\)</span>, there exists a <span class="math">\(k\in \{0,1,\ldots, N-1\}\)</span> such that <span class="math">\(x\in [x_k, x_{k+1})\)</span>, we have
</p><div class="math" >\begin{eqnarray}
|f(x) - s_\varepsilon(x)| 
&amp;=&amp; |f(x)-f(x_k) + f(x_k)-s_\varepsilon(x)|\\
&amp;\le&amp; |f(x)-f(x_k)| \\
&amp;<&amp; \varepsilon.
\end{eqnarray}<div style="text-align:right;">(6.1)</div></div><p>
For <span class="math">\(x=1\)</span>, the above estimate holds obviously.</p><p>Thus, we have
</p><div class="math">\[
\|f - s_\varepsilon\|_{\infty,[a,b]} < \varepsilon.
\]</div><p>
</div></p><p>We next prove the denseness of the set of all shallow Heaviside networks.</p><p><div class="lemma" id="denseness.shallow.heaviside.networks.lemm">
<div class="heading-container">
<b class="heading">Lemma:</b>
</div>
[<b>Expressivity of Shallow Heaviside Networks</b>]
</p><p>For every <span class="math">\(f \in \mathbb{C}([a,b])\)</span> and every <span class="math">\(\varepsilon > 0\)</span>, there exists <span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}} \in \mathscr{N}_{2}(H; 1, 1)\)</span>, for some finite <span class="math">\(\hat{n}_\varepsilon\in \mathbb{N}\)</span>, such that
</p><div class="math">\[
\|f - {\mathscr{F}_{2,\hat{n}_\varepsilon}}\|_{\infty,[a,b]} < \varepsilon.
\]</div><p>
where <span class="math">\(\mathscr{A}=H\)</span> is the Heaviside activation function.
</div></p><p><div class="proofs">
<div class="heading-container">
<b class="heading">Proof:</b>
</div>
Every <span class="math">\({\mathscr{F}_{2,\hat{n}}} \in \mathscr{N}_{2}(H; 1, 1)\)</span> takes the form
</p><div class="math">\[
{\mathscr{F}_{2,\hat{n}}}(x) =  \sum_{j=1}^{\hat{n}}w^{(2)}_{1j}\, H\left(w^{(1)}_{j1} x - b^{(1)}_j\right) - b^{(2)}_1, x\in \mathbb{R},
\]</div><p>
for some <span class="math">\(\hat{n}\in \mathbb{N}\)</span>, and <span class="math">\(w^{(1)}_{j1}, b^{(1)}_{j}, w^{(2)}_{1j}, b^{(2)}_{1} \in \mathbb{R},\)</span> for <span class="math">\(l=1,2,\)</span>, and <span class="math">\(j=1,2,\ldots, \hat{n}\)</span>.</p><p>Let <span class="math">\(f \in \mathbb{C}([a,b])\)</span> and <span class="math">\(\varepsilon > 0\)</span> be given. We have to find <span class="math">\(\hat{n}_\varepsilon\in \mathbb{N}\)</span> and  augmented weights <span class="math">\(\overline{W}^{(1)}\in \mathbb{R}^{\hat{n}_\varepsilon \times 2}\)</span> and <span class="math">\(\overline{W}^{(2)}\in \mathbb{R}^{1 \times (\hat{n}_\varepsilon +1)}\)</span> such that the corresponding ANN <span class="math">\({\mathscr{F}_{2,\hat{n}}} \in \mathscr{N}_{2}(H; 1, 1)\)</span> satisfies the required inequality.</p><p>Consider the step function <span class="math">\(s_\varepsilon:[a,b]\to\mathbb{R}\)</span> defined in the proof of <a href="HTMLExpressivityoverFunctionSpaces.html#denseness.simple.functions.lemm">Lemma &laquo;Click Here&raquo; </a>. We now express this step function <span class="math">\(s_\varepsilon\)</span> as a linear combination of shifted Heaviside functions.  For any <span class="math">\(x\in [a,b)\)</span>, there exists a <span class="math">\(k\in \{0,1,\ldots, N-1\}\)</span> such that <span class="math">\(x\in [x_k, x_{k+1})\)</span>. Thus, we have
</p><div class="math">\[
H(x-x_j) = \left\{\begin{array}{cc}
				1,&amp;\text{if } j\le k\\
				0,&amp;\text{if } j> k,
			\end{array}\right.
~\text{for}~j=0,1,\ldots, N.
\]</div><p>
Thus, we can write the step function <span class="math">\(s_\varepsilon\)</span> as
</p><div class="math">\[
s_\varepsilon(x) = \sum_{j=0}^{N-1} c_j H(x - x_j), ~x\in [a,b),
\]</div><p>
where <span class="math">\(c_j \in \mathbb{R}\)</span> are defined as
</p><div class="math">\[
c_0=f(x_0), ~c_j=f(x_j) - \sum_{i=0}^{j-1}c_i,~j=1,2,\ldots, N-1.
\]</div><p>
Let us take <span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}} = s_\varepsilon\)</span>, with <span class="math">\(\hat{n}_\varepsilon = N\)</span>, <span class="math">\(w^{(1)}_{j1}=1,\)</span> <span class="math">\(b^{(1)}_j = x_{j-1}\)</span>, <span class="math">\(w^{(2)}_{1j}=c_{j-1},\)</span> <span class="math">\(j=1,2,\ldots, \hat{n}_\varepsilon\)</span>, and <span class="math">\(b^{(2)}_1 = 0\)</span>.</p><p>Then, we see that <span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}}\in \mathscr{N}_{2}(H; 1, 1)\)</span> and from the estimate for <span class="math">\(s_\varepsilon\)</span> obtain in the proof of <a href="HTMLExpressivityoverFunctionSpaces.html#denseness.simple.functions.lemm">Lemma &laquo;Click Here&raquo; </a>, we see that
</p><div class="math">\[
\|f - {\mathscr{F}_{2,\hat{n}_\varepsilon}}\|_{\infty, [a,b]} < \varepsilon.
\]</div><p>
This proves the lemma.
</div></p><p>Next, let us extend our discussion to the denseness of continuous piecewise affine functions.</p><p><div class="lemma" id="denseness.continuous.piecewise.affine.functions.lemm">
<div class="heading-container">
<b class="heading">Lemma:</b>
</div>
[<b>Denseness of Continuous Piecewise Affine Functions</b>]
</p><p>For every <span class="math">\(f \in \mathbb{C}([a,b])\)</span> and every <span class="math">\(\varepsilon > 0\)</span>, there exists a continuous piecewise affine function <span class="math">\(\varphi_\varepsilon: [a,b]\rightarrow \mathbb{R}\)</span> such that
</p><div class="math">\[
\|f - \varphi_\varepsilon\|_{\infty,[a,b]} < \varepsilon.
\]</div><p>
</div></p><p><div class="proofs">
<div class="heading-container">
<b class="heading">Proof:</b>
</div>
Let <span class="math">\(f \in \mathbb{C}([a,b])\)</span> and <span class="math">\(\varepsilon>0\)</span> be given.</p><p>By uniform continuity of <span class="math">\(f\)</span> on <span class="math">\([a,b]\)</span>, there exists a <span class="math">\(\delta=\delta(\varepsilon)>0\)</span> and a <span class="math">\(N>0\)</span> is sufficiently large with <span class="math">\((b-a)/N<\delta(\varepsilon)\)</span>,  such that on the uniform partition 
</p><div class="math">\[
a = x_0 < x_1 < \dots < x_N = b,
\]</div><p>
we have
</p><div class="math">\[
|f(x) - f(y)| < \varepsilon, \quad \text{for all } x,y \in [x_k, x_{k+1}],~k=0,\dots,N-1.
\]</div><p>
Define the piecewise affine function
</p><div class="math">\[
\varphi_\varepsilon(x) = f(x_k) + t\big(f(x_{k+1}) - f(x_{k})\big),~ \text{for}~ x=x_k+t (x_{k+1}-x_k),~\text{with}~t\in [0,1],
\]</div><p>
for <span class="math">\(k=0,1,\ldots, N-1\)</span>.</p><p>Since <span class="math">\(\varphi_\varepsilon(x)\)</span> lies between <span class="math">\(f(x_k)\)</span> and <span class="math">\(f(x_{k+1})\)</span> for any <span class="math">\(x\in [x_k, x_{k+1}]\)</span>, and <span class="math">\(f\)</span> is continuous, by intermediate value theorem, we can find a <span class="math">\(\xi\in [x_k, x_{k+1}]\)</span> such that
</p><div class="math">\[
\varphi_\varepsilon(x) = f(\xi).
\]</div><p>
Now let us take <span class="math">\(x\in [a,b]\)</span> arbitrarily. We can find a <span class="math">\(k\in \{0,1,\ldots, N-1\}\)</span> such that <span class="math">\(x\in [x_k, x_{k+1}]\)</span> and consequently, we have
</p><div class="math">\[
|f(x) - \varphi_\varepsilon(x)| = |f(x) - f(\xi)|,
\]</div><p>
for some <span class="math">\(\xi\in [x_k, x_{k+1}]\)</span>. Since <span class="math">\(x,\xi\in [x_k, x_{k+1}]\)</span>, by the way we have chosen the partition, we see that
</p><div class="math">\[
|f(x) - \varphi_\varepsilon(x)|= |f(x) - f(\xi)| < \varepsilon.
\]</div><p>
Since the above estimate holds for any <span class="math">\(x\in [a,b]\)</span>, we have
</p><div class="math">\[
\|f-\varphi_\varepsilon\|_{\infty, [a,b]} = \sup_{x\in [a,b]} |f(x) - \varphi_\varepsilon(x)| < \varepsilon.
\]</div><p>
</div></p><p>We now prove that the set of shallow <span class="math">\(\texttt{ReLU}\)</span> networks is a universal approximator of <span class="math">\(\mathbb{C}(\mathbb{R})\)</span>. As we did in the above case, we achieve this in two steps.  The first step is stated in the following lemma.</p><p><div class="lemma" id="denseness.shallow.ReLU.networks.lemm">
<div class="heading-container">
<b class="heading">Lemma:</b>
</div>
[<b>Denseness of Shallow ReLU Networks</b>]
</p><p>For every <span class="math">\(f \in \mathbb{C}([a,b])\)</span> and every <span class="math">\(\varepsilon > 0\)</span>, there exists <span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}} \in \mathscr{N}_{2}(\texttt{ReLU}; 1, 1)\)</span>, for some finite <span class="math">\(\hat{n}_\varepsilon\in \mathbb{N}\)</span>, such that
</p><div class="math">\[
\|f - {\mathscr{F}_{2,\hat{n}_\varepsilon}}\|_{\infty,[a,b]} < \varepsilon.
\]</div><p>
</div></p><p><div class="proofs">
<div class="heading-container">
<b class="heading">Proof:</b>
</div>
Every <span class="math">\({\mathscr{F}_{2,\hat{n}}} \in \mathscr{N}_{2}(\texttt{ReLU}; 1, 1)\)</span> takes the form
</p><div class="math">\[
{\mathscr{F}_{2,\hat{n}}}(x) =  \sum_{j=1}^{\hat{n}}w^{(2)}_{1j}\, \texttt{ReLU}\left(w^{(1)}_{j1} x - b^{(1)}_j\right) - b^{(2)}_1, x\in \mathbb{R},
\]</div><p>
for some <span class="math">\(\hat{n}\in \mathbb{N}\)</span>, and <span class="math">\(w^{(1)}_{j1}, b^{(1)}_{j}, w^{(2)}_{1j}, b^{(2)}_{1} \in \mathbb{R},\)</span> for <span class="math">\(l=1,2,\)</span>, and <span class="math">\(j=1,2,\ldots, \hat{n}\)</span>.</p><p>Let <span class="math">\(f \in \mathbb{C}([a,b])\)</span> and <span class="math">\(\varepsilon > 0\)</span> be given. We have to find <span class="math">\(\hat{n}_\varepsilon\in \mathbb{N}\)</span> and  augmented weights <span class="math">\(\overline{W}^{(1)}\in \mathbb{R}^{\hat{n}_\varepsilon \times 2}\)</span> and <span class="math">\(\overline{W}^{(2)}\in \mathbb{R}^{1 \times (\hat{n}_\varepsilon +1)}\)</span> such that the corresponding ANN <span class="math">\({\mathscr{F}_{2,\hat{n}}} \in \mathscr{N}_{2}(\texttt{ReLU}; 1, 1)\)</span> satisfies the required inequality.</p><p>Choose the partition 
</p><div class="math">\[
a = x_0 < x_1 < \dots < x_N = b,
\]</div><p>
with <span class="math">\((b-a)/N<\delta(\varepsilon)\)</span> as in the proof of <a href="HTMLExpressivityoverFunctionSpaces.html#denseness.continuous.piecewise.affine.functions.lemm">Lemma &laquo;Click Here&raquo; </a>.</p><p>Let us take <span class="math">\(\hat{n}_\varepsilon = N\)</span>, <span class="math">\(w^{(1)}_{j1}=1,\)</span> <span class="math">\(b^{(1)}_j = x_{j-1}\)</span>.
Then, we have
</p><div class="math">\[
{\mathscr{F}_{2,\hat{n}_\varepsilon}}(x) =  \sum_{j=1}^{\hat{n}_\varepsilon}w^{(2)}_{1j}\, \texttt{ReLU}\left(x - x_{j-1}\right) - b^{(2)}_1, x\in \mathbb{R},
\]</div><p>
The distributional (weak) derivative of <span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}}\)</span> is a simple function (how?). Therefore, <span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}}\)</span> is a continuous piecewise affine function. Hence, from <a href="HTMLExpressivityoverFunctionSpaces.html#denseness.continuous.piecewise.affine.functions.lemm">Lemma &laquo;Click Here&raquo; </a>, it is enough to choose the weights <span class="math">\(w^{(2)}_{1j}\)</span>, <span class="math">\(j=1,2,\ldots, \hat{n}_\varepsilon\)</span> and the bias <span class="math">\(b^{(2)}_1\)</span> such that 
</p><div class="math">\[
{\mathscr{F}_{2,\hat{n}_\varepsilon}}(x) = \varphi_\varepsilon(x),~ \text{for all}~ x\in [a,b].
\]</div><p>From the proof of <a href="HTMLExpressivityoverFunctionSpaces.html#denseness.continuous.piecewise.affine.functions.lemm">Lemma &laquo;Click Here&raquo; </a>, we define the piecewise affine function
</p><div class="math">\[
\varphi_\varepsilon(x) = f(x_k) + \left(\frac{f(x_{k+1}) - f(x_{k})}{x_{k+1}-x_k}\right)(x-x_k).
\]</div><p>
for <span class="math">\(k=0,1,\ldots, N-1\)</span>.</p><p><b>Case 0:</b> Since
<span class="math">\(
\varphi_\varepsilon(x_0) = f(x_0)
\)</span>
and <span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}}(x_0) = -b^{(2)}_1,\)</span> we choose <span class="math">\(b^{(2)}_1=-f(x_0)\)</span>.</p><p><b>Case 1:</b> Since
<span class="math">\(
\varphi_\varepsilon(x_1) = f(x_1)
\)</span>
and 
</p><div class="math">\[
{\mathscr{F}_{2,\hat{n}_\varepsilon}}(x_1) = w^{(2)}_{11}\, \texttt{ReLU}\left(x_1 - x_0\right) + f(x_0)
= w^{(2)}_{11}\, (x_1 - x_0) + f(x_0),
\]</div><p>
by equating the above two expressions, and noting that the uniform partition implies <span class="math">\(x_{k+1}-x_k = (b-a)/\hat{n}_\varepsilon\)</span>, we have 
</p><div class="math">\[
w^{(2)}_{11} = \frac{\hat{n}_\varepsilon}{b-a} \big(f(x_{1}) - f(x_{0})\big).
\]</div><p>
<b>Case 2:</b> Similarly,
<span class="math">\(
\varphi_\varepsilon(x_2) = {\mathscr{F}_{2,\hat{n}_\varepsilon}}(x_2)
\)</span>
implies
</p><div class="math">\[
w^{(2)}_{12}
=
\frac{\hat{n}_\varepsilon\big(f(x_{2}) - 2f(x_{1})+f(x_0)\big)}{b-a}.
\]</div><p>
and so on.</p><p><b>General Case:</b> Continuing in this way, we can get
</p><div class="math">\[
w^{(2)}_{1j}
=
\frac{\hat{n}_\varepsilon}{b-a} \big(f(x_{j}) - 2f(x_{j-1})+f(x_{j-2})\big),
~~j=2,3,\ldots,\hat{n}_\varepsilon.
\]</div><p>
Thus, we obtained all the weights and biases so that <span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}}(x_k)=\varphi(x_k)\)</span>, for <span class="math">\(k=0,1,\ldots,  \hat{n}_\varepsilon.\)</span> Since both these functions are piecewise linear, they are equal for all <span class="math">\(x\in [a,b]\)</span>.
Thus, the required estimate follows from <a href="HTMLExpressivityoverFunctionSpaces.html#denseness.continuous.piecewise.affine.functions.lemm">Lemma &laquo;Click Here&raquo; </a>.
</div></p><p><div class="theorem">
<div class="heading-container">
<b class="heading">Theorem:</b>
</div>
[<b>Universal Approximation by Shallow ReLU Networks</b>]</p><p>The class <span class="math">\(\mathscr{N}_{2}(\texttt{ReLU}; 1, 1)\)</span> is a universal approximator of <span class="math">\(\mathbb{C}(\mathbb{R})\)</span>.
</div></p><p><div class="proofs">
<div class="heading-container">
<b class="heading">Proof:</b>
</div>
Let <span class="math">\(\mathcal{X}\subset\mathbb{R}\)</span> be compact and let <span class="math">\(f\in\mathbb{C}(\mathcal{X})\)</span> and
<span class="math">\(\varepsilon>0\)</span> be given.</p><p>We have to show that there exists
<span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}}\in\mathscr{N}_{2}(\texttt{ReLU};1,1)\)</span> such that
</p><div class="math">\[
\|f-{\mathscr{F}_{2,\hat{n}_\varepsilon}}\|_{\infty,\mathcal{X}}<\varepsilon.
\]</div><p> Since <span class="math">\(\mathcal{X}\)</span> is compact it is closed in <span class="math">\(\mathbb{R}\)</span>. Since <span class="math">\(f\)</span> is continuous on <span class="math">\(\mathcal{X}\)</span>,
 by the Tietze extension theorem there exists a continuous function
<span class="math">\(\widetilde f\)</span> on some closed and bounded interval <span class="math">\([a,b]\)</span> with <span class="math">\(\mathcal{X}\subset[a,b]\)</span>
such that <span class="math">\(\widetilde f|_{\mathcal{X}} = f\)</span>.</p><p>By <a href="HTMLExpressivityoverFunctionSpaces.html#denseness.shallow.ReLU.networks.lemm">Lemma &laquo;Click Here&raquo; </a>, we can obtain an ANN <span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}}\in\mathscr{N}_{2}(\texttt{ReLU};1,1)\)</span> with width <span class="math">\(\hat{n}_\varepsilon=N\)</span> that satisfies the estimate
</p><div class="math">\[
\|\widetilde{f}-{\mathscr{F}_{2,\hat{n}_\varepsilon}}\|_{\infty,[a,b]}<\varepsilon.
\]</div><p>Restricting this estimate to <span class="math">\(\mathcal{X}\subset[a,b]\)</span> yields
</p><div class="math" >\begin{eqnarray}
\|f-{\mathscr{F}_{2,\hat{n}_\varepsilon}}\|_{\infty,\mathcal{X}} 
&amp;=&amp; \sup_{x\in\mathcal{X}} |\widetilde f(x)-{\mathscr{F}_{2,\hat{n}_\varepsilon}}(x)|\\
&amp;\le&amp; \sup_{x\in[a,b]} |\widetilde f(x)-{\mathscr{F}_{2,\hat{n}_\varepsilon}}(x)| \\
&amp;=&amp;   \|\widetilde{f}-{\mathscr{F}_{2,\hat{n}_\varepsilon}}\|_{\infty,[a,b]}< \varepsilon.
\end{eqnarray}<div style="text-align:right;">(6.2)</div></div><p>
Thus <span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}}\in\mathscr{N}_2(\texttt{ReLU};1,1)\)</span> provides the required uniform approximation on
<span class="math">\(\mathcal{X}\)</span>. Since <span class="math">\(\mathcal{X}\)</span> and <span class="math">\(f\)</span> were arbitrary, <span class="math">\(\mathscr{N}_2(\texttt{ReLU};1,1)\)</span>
is dense in <span class="math">\(C(\mathcal{X})\)</span> for every compact <span class="math">\(\mathcal{X}\subset\mathbb{R}\)</span>,
i.e. it is a universal approximator of <span class="math">\(C(\mathbb{R})\)</span>.
</div></p><p>Next, we prove the denseness of shallow sigmoid networks. We discuss this result in a general sense by introducing
the sigmoidal functions in which the logistic function is a particular case.</p><p><div class="definition">
<div class="heading-container">
<b class="heading">Definition:</b>
</div>
[<b>Sigmoidal Function</b>]</p><p>A function <span class="math">\(\varphi:\mathbb{R}\to\mathbb{R}\)</span> is said to be <b>sigmoidal</b> if 
</p><div class="math" >\begin{eqnarray}
\lim_{t\to -\infty}\varphi(t) = 0, 
\qquad 
\lim_{t\to +\infty}\varphi(t) = 1.
\end{eqnarray}<div style="text-align:right;">(6.3)</div></div><p>
</div></p><p><div class="note">
<div class="heading-container">
<b class="heading">Note:</b>
</div>
The logistic functions <span class="math">\(\mathscr{S}_k\)</span> are sigmoidal functions for any <span class="math">\(k>0\)</span>.
</div></p><p>We now prove denseness of shallow sigmoidal networks under stronger assumptions  than needed in order to make the proof simple.</p><p><div class="lemma" id="denseness.shallow.sigmoid.networks.lemm">
<div class="heading-container">
<b class="heading">Lemma:</b>
</div>
[<b>Denseness of Shallow Sigmoid Networks</b>]
</p><p>Let <span class="math">\(\mathscr{S}:\mathbb{R}\rightarrow \mathbb{R}\)</span> be a continuously differentiable monotonically increasing sigmoidal function, where <span class="math">\(\mathscr{S}'\)</span> is bounded in <span class="math">\(\mathbb{R}\)</span>.</p><p>For every <span class="math">\(f \in \mathbb{C}([a,b])\)</span> and every <span class="math">\(\varepsilon > 0\)</span>, there exists <span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}} \in \mathscr{N}_{2}(\mathscr{S}; 1, 1)\)</span>, for some finite <span class="math">\(\hat{n}_\varepsilon\in \mathbb{N}\)</span>, such that
</p><div class="math">\[
\|f - {\mathscr{F}_{2,\hat{n}_\varepsilon}}\|_{\infty,[a,b]} < \varepsilon.
\]</div><p>
</div></p><p><div class="proofs">
<div class="heading-container">
<b class="heading">Proof:</b>
</div>
Given <span class="math">\(f \in \mathbb{C}([a,b])\)</span> and <span class="math">\(\varepsilon>0\)</span>.</p><p>By <a href="HTMLExpressivityoverFunctionSpaces.html#denseness.simple.functions.lemm">Lemma &laquo;Click Here&raquo; </a>, we can find a simple function 
</p><div class="math">\[
s_\varepsilon(x) =  \sum_{i=0}^{N-1} c_i H(x - x_i),
~\text{with}~ |c_i|\le \frac{\varepsilon}{4} ~\text{(why?)}
\]</div><p> 
 such that
</p><div class="math">\[
\|f - s_\varepsilon\|_{\infty,[a,b]} < \frac{\varepsilon}{2}.
\]</div><p>
For a fixed <span class="math">\(\alpha>0\)</span>, define
</p><div class="math">\[
s_\alpha = (s_\varepsilon \ast \psi_\alpha) (x),
\]</div><p>
where 
</p><div class="math">\[
\psi_\alpha(x) =  \frac{1}{\alpha}\mathscr{S}'\!\left(\frac{x}{\alpha}\right) =: \mathscr{S}'_\alpha(x),
\qquad \alpha > 0.
\]</div><p>
For any fixed <span class="math">\(\alpha>0\)</span>, since
<span class="math">\(
\int\limits_{-\infty}^{\infty} \psi_\alpha(x) dx = 1,
\)</span>
we can write
</p><div class="math" >\begin{eqnarray}
s_\epsilon(x) - s_\alpha(x) 
&amp;=&amp;  \int\limits_{-\infty}^{\infty} \psi_\alpha(y)\big(s_\epsilon(x) - s_\epsilon(x-y) \big) dy.
\end{eqnarray}<div style="text-align:right;">(6.4)</div></div><p>
Further, for any <span class="math">\(\epsilon'>0\)</span>, we can write the above integral as
</p><div class="math">\[
s_\epsilon(x) - s_\alpha(x) =
\left(
\int\limits_{-\infty}^{-\epsilon'} + \int\limits_{-\epsilon'}^{\epsilon'} +  \int\limits_{\epsilon'}^{\infty}\right) \psi_\alpha(y)\big(s_\epsilon(x) - s_\epsilon(x-y) \big) dy
\]</div><p>
Let us denote the three integrals on the right hand side of the above equation as <span class="math">\(I_k\)</span>, <span class="math">\(k=1,2,3\)</span>, and take the modulus on both sides to get
</p><div class="math">\[
|s_\epsilon(x) - s_\alpha(x)| \le |I_1| + |I_2| + |I_3|.
\]</div><p>
It can be shown that <span class="math">\(\psi_\alpha \rightarrow \delta\)</span> as <span class="math">\(\alpha \rightarrow 0\)</span> in the weak sense (in the sense of distribution). Therefore, <span class="math">\(\psi_\alpha \rightarrow 0\)</span> as <span class="math">\(\alpha \rightarrow 0\)</span> pointwise on <span class="math">\((-\infty, \epsilon')\)</span> and also on <span class="math">\((\epsilon', \infty)\)</span>. Further, since <span class="math">\(\psi_\alpha\)</span> is integrable on <span class="math">\(\mathbb{R}\)</span> and <span class="math">\(s_\varepsilon\)</span> is bounded, by Dominated Convergence Theorem, we see that <span class="math">\(I_1\rightarrow 0\)</span> and <span class="math">\(I_2\rightarrow 0\)</span> as <span class="math">\(\alpha\rightarrow 0\)</span>.  Therefore, we can see that
</p><div class="math">\[
|I_1| +  |I_3| < \frac{\varepsilon}{4}, ~ \text{for sufficiently small}~\alpha.
\]</div><p>
On the other hand, by choosing <span class="math">\(\epsilon'<(b-a)/N\)</span>, we can see that
</p><div class="math" >\begin{eqnarray}
|I_2| &amp;\le&amp; 
\int\limits_{-\epsilon'}^{\epsilon'} \psi_\alpha(y)\big|s_\epsilon(x) - s_\epsilon(x-y) \big| dy \\
&amp;\le&amp; |c_k|\int\limits_{-\epsilon'}^{\epsilon'}  \psi_\alpha(y) dy,~~\text{for some }~k\in \{1,2,\ldots, N\}\\
&amp;\le&amp; \frac{\varepsilon}{4}\int\limits_{-\infty}^{\infty}  \psi_\alpha(y) dy\\
&amp;=&amp;\frac{\varepsilon}{4}.
\end{eqnarray}<div style="text-align:right;">(6.5)</div></div><p>
Combining the estimates of the three integrals, we get
</p><div class="math">\[
|s_\epsilon(x) - s_\alpha(x)| \le \frac{\varepsilon}{2},~\text{for all}~ x\in [a,b].
\]</div><p>
Thus, we have
</p><div class="math">\[
\|s_\epsilon - s_\alpha\|_{\infty,[a,b]} < \frac{\varepsilon}{2}.
\]</div><p>
Combining the two inequalities, we get
</p><div class="math">\[
\|f - s_\alpha\|_{\infty,[a,b]} < \varepsilon.
\]</div><p>
It is enough to show that <span class="math">\(s_\alpha\in \mathscr{N}_{2}(\mathscr{S}; 1, 1)\)</span>.</p><p>Up on using properties of convolution operator, we get
</p><div class="math" >\begin{eqnarray}
s_\alpha(x)
&amp;=&amp; (s_\varepsilon * \psi_\alpha)(x)\\
&amp;=&amp; \left(s_\varepsilon *\mathscr{S}_{\alpha}'\right)(x)\\
&amp;=&amp; \left(s_\varepsilon' *\mathscr{S}_{\alpha}\right)(x)\\
&amp;=&amp; \sum_{i=0}^{N-1} c_i \frac{d}{dx}H(x - x_i)*\mathscr{S}_{\alpha}(x),
\end{eqnarray}<div style="text-align:right;">(6.6)</div></div><p>
where the derivative in the above expression is taken in the distribution sense.  Noting that, in the distributional sense
</p><div class="math">\[
\frac{d}{dx}H(x - x_i) = \delta(x-x_i) =  \delta_{x_i}(x),
\]</div><p>
we get
</p><div class="math" >\begin{eqnarray}
s_\alpha(x)
&amp;=&amp; \left(\sum_{i=0}^{N-1} c_i \delta_{x_i}*\mathscr{S}_{\alpha}\right)(x)\\
&amp;=&amp; \sum_{i=0}^{N-1} c_i \left(\delta_{x_i}*\mathscr{S}_{\alpha}\right)(x)\\
&amp;=&amp; \sum_{i=0}^{N-1} c_i \mathscr{S}_{\alpha}(x-x_\alpha)\\
&amp;=&amp; \sum_{i=0}^{N-1} c_i\mathscr{S}\left(\frac{x-x_i}{\alpha}\right).
\end{eqnarray}<div style="text-align:right;">(6.7)</div></div><p>
Since any <span class="math">\({\mathscr{F}_{2,\hat{n}_\varepsilon}}\in \mathscr{N}_{2}(\mathscr{S}; 1, 1)\)</span> is of the form
</p><div class="math">\[
{\mathscr{F}_{2,\hat{n}_\varepsilon}}(x)  
=  \sum_{j=1}^{\hat{n}}w^{(2)}_{1j}\, \mathscr{S}\left(w^{(1)}_{j1} x - b^{(1)}_j\right) - b^{(2)}_1, x\in \mathbb{R},
\]</div><p>
By choosing <span class="math">\(\hat{n}_\varepsilon = N\)</span>, and <span class="math">\(w^{(1)}_{j1}=\frac{1}{\alpha}\)</span>, <span class="math">\(b^{(1)}_{j}=\frac{x_{j-1}}{\alpha}\)</span>, 
<span class="math">\(w^{(2)}_{1j}=c_{j-1}\)</span>, and <span class="math">\(b^{(2)}_{1}=0\)</span>, <span class="math">\(j=1,2,\ldots, N\)</span>, we see that
<span class="math">\(s_\varepsilon = {\mathscr{F}_{2,\hat{n}_\varepsilon}}\in \mathscr{N}_{2}(\mathscr{S}; 1, 1)\)</span>.
This completes the proof.
</div></p><p>We also have the universal approximation theorem for sigmoidal networks</p><p><div class="theorem">
<div class="heading-container">
<b class="heading">Theorem:</b>
</div>
[<b>Universal Approximation by Shallow Sigmoidal Networks</b>]</p><p>The class <span class="math">\(\mathscr{N}_{2}(\mathscr{S}; 1, 1)\)</span> is a universal approximator of <span class="math">\(\mathbb{C}(\mathbb{R})\)</span>.
</div></p><p>Proof is left as an exercise.</p><p>We can extend the universal approximation property of shallow networks to multi-dimensional, <i>i.e.,</i> to the class <span class="math">\(\mathscr{N}_{2}(\mathscr{S}; n_0, 1)\)</span>, for any <span class="math">\(n_0\ge 1\)</span>. The theorem is called the Cybenko theorem, which we state here and omit the proof for this course.</p><p><div class="theorem">
<div class="heading-container">
<b class="heading">Theorem:</b>
</div>
[<b>Cybenko Theorem</b>]</p><p>Let <span class="math">\(\mathscr{S}:\mathbb{R}\rightarrow \mathbb{R}\)</span> be a continuous sigmoidal function
and let <span class="math">\(I_{n_0}=[0,1]^{n_0}\)</span>.
Then, <span class="math">\(\mathscr{N}_{2}(\mathscr{S}; n_0, 1)\)</span>
is dense in <span class="math">\(C(I_{n_0})\)</span>.
</div></p><p><div class="note">
<div class="heading-container">
<b class="heading">Note:</b>
</div>
Proof of the above theorem is omitted for this course.  Interested students can refer to Theorem 9.3.6 (page 259) in the following book:</p><p>Calin, Ovidiu, <i>Deep Learning Architectures: A Mathematical Approach</i>, Springer,	2020.</p><p><a href="https://link.springer.com/book/10.1007/978-3-030-36721-3" style="color: #008080; font-family: monospace;">
  Click here to see the details of the book
</a>
</div></p><p>The following theorem categorizes the activation functions that can lead to universal approximators within shallow networks.</p><p><div class="theorem">
<div class="heading-container">
<b class="heading">Theorem:</b>
</div>
The class of shallow networks <span class="math">\(\mathscr{N}_{2}(\mathscr{A}; n_0, 1)\)</span> is a universal approximator of <span class="math">\(\mathbb{C}(\mathbb{R}^{n_0})\)</span> if and only if <span class="math">\(\mathscr{A}\)</span> is not a polynomial.
</div></p><p><div class="note">
<div class="heading-container">
<b class="heading">Note:</b>
</div>
Proof of the above theorem is omitted for this course.  Interested students can refer to Theorem 3.8 (page 27) in the following Lecture Notes:</p><p>Petersen, Philipp and Zech, Jakob, <i>Mathematical theory of deep learning</i>, 2025.</p><p><a href="https://arxiv.org/abs/2407.18384" style="color: #008080; font-family: monospace;">
arXiv:2407.18384 [cs.LG]
</a>
</div></p><p><div class="note">
<div class="heading-container">
<b class="heading">Note:</b>
</div>
From the above results, we see that depth is not strictly necessary for universal approximation. However, deeper architectures can achieve comparable approximation accuracy with exponentially fewer neurons in certain cases. Hence, depth contributes to efficiency rather than expressivity. Universal approximation results have also been established for deep networks, often with milder conditions on the activation function.
</div></p><p><h2 id="feature-mapping-perspective">Feature Mapping Perspective</h2></p><p>Having discussed the universal approximation capacity of shallow networks and noting that this expressive power also extends to deep networks, we now ask the question of

<i>What makes neural networks so expressive?</i>

An answer to this question may be to view the hidden layers as performing a nonlinear feature mapping of the input space.</p><p>Just for notational convenience, let us restrict our discussions in this subsection to the set <span class="math">\(\mathscr{N}_{2}(\mathscr{A}, n_0, 1)\)</span>, which can be extended to any shallow or deep networks in a similar and straightforward way.</p><p>Any function <span class="math">\({\mathscr{F}_{2,\hat{n}}}\in \mathscr{N}_{2}(\mathscr{A}, n_0,1)\)</span> is of the form
</p><div class="math">\[
{\mathscr{F}_{2,\hat{n}}}(\boldsymbol{x}) =  \sum_{j=1}^{\hat{n}}w^{(2)}_{1j}\, \mathscr{A}\left(\boldsymbol{w}^{(1)}_{j}\cdot \boldsymbol{x} - b^{(1)}_j\right) - b^{(2)}_1, \boldsymbol{x}\in \mathbb{R}^{n_0}.
\]</div><p>
Let us denote the activation from the hidden layer as
</p><div class="math">\[
\boldsymbol{\phi}(x) = \left( 
		\mathscr{A}\left(\boldsymbol{w}^{(1)}_{1}\cdot \boldsymbol{x} - b^{(1)}_1\right),
		 \mathscr{A}\left(\boldsymbol{w}^{(1)}_{2}\cdot \boldsymbol{x} - b^{(1)}_2\right), 
		 \cdots, 
		 \mathscr{A}\left(\boldsymbol{w}^{(1)}_{\hat{n}}\cdot \boldsymbol{x} - b^{(1)}_{\hat{n}}\right) 
		 \right).
\]</div><p>
Then, we can write the network function as
</p><div class="math" id="ann.kernel.form.eq">\begin{eqnarray}
{\mathscr{F}_{2,\hat{n}}}(x) = \boldsymbol{w}^{(2)}\cdot \boldsymbol{\phi}(x) - b^{(2)}_1,~\boldsymbol{x}\in \mathbb{R}^{n_0}.
\end{eqnarray}<div style="text-align:right;">(6.8)</div></div><p>
This shows that the hidden layer plays the role of the feature mapping that transforms the input to a feature space which is taken as the input space for the output layer.</p><p>In traditional machine learning, a feature map is often chosen manually or implicitly through a kernel function (as in SVMs). Whereas, neural networks learn the feature map automatically from the dataset by adjusting the parameters of the hidden layer through optimization.  </p><p><h3 id="connection-to-kernel-machines">Connection to kernel machines</h3></p><p>We now compare the feature-mapping viewpoint of shallow networks with the general form of a kernel machine.</p><p>Let us first recall the general form of a kernel machine.</p><p>For an input space <span class="math">\(\mathcal{X}\subseteq \mathbb{R}^{n_0}\)</span>, let <span class="math">\(\text{𝕜}:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\)</span> be a symmetric positive definite kernel and let <span class="math">\(\mathbb{H}_\text{𝕜}\)</span> be a Hilbert space (called <b>Reproducing Kernel Hilbert Spaces</b>, RKHS) with feature map <span class="math">\(\boldsymbol{\phi}_\text{𝕜}:\mathcal{X}\to\mathbb{H}_\text{𝕜}\)</span> satisfying
</p><div class="math">\[
\text{𝕜}(\boldsymbol{x}_1,\boldsymbol{x}_2)=\Big\langle\boldsymbol{\phi}_\text{𝕜}(\boldsymbol{x}_1),\boldsymbol{\phi}_\text{𝕜}(\boldsymbol{x}_2)\Big\rangle_{\mathbb{H}_\text{𝕜}}.
\]</div><p>
A <b>linear kernel machine</b> (for supervised learning) produces predictions of the form
</p><div class="math" >\begin{eqnarray}
\mathscr{F}(\boldsymbol{x}) &amp;=&amp; \sum_{i=1}^N \alpha_i\,\text{𝕜}(\boldsymbol{x}_i,\boldsymbol{x}) + b\\
       &amp;=&amp; \Big\langle \sum_{i=1}^N \alpha_i \boldsymbol{\phi}_\text{𝕜}(\boldsymbol{x}_i),\, \boldsymbol{\phi}_\text{𝕜}(\boldsymbol{x})\Big\rangle_{\mathbb{H}_\text{𝕜}} + b,
\end{eqnarray}<div style="text-align:right;">(6.9)</div></div><p>
where \(\{\boldsymbol{x}_i\}_{i=1}^N\) are the training inputs and the coefficients \(\{\alpha_i\}\) are obtained by minimizing a chosen cost function with penalty.  </p><p>By taking 
</p><div class="math">\[
\boldsymbol{w} = \sum_{i=1}^N \alpha_i \boldsymbol{\phi}_\text{𝕜}(\boldsymbol{x}_i)
\]</div><p>
we can write the predictions of a linear kernel machine as
</p><div class="math">\[
\mathscr{F}(\boldsymbol{x}) = \Big\langle \boldsymbol{w},\, \boldsymbol{\phi}_\text{𝕜}(\boldsymbol{x})\Big\rangle_{\mathbb{H}_\text{𝕜}} + b.
\]</div><p><div class="note">
<div class="heading-container">
<b class="heading">Note:</b>
</div>
Recall, SVM solve a hinge-loss quadratic programming problem whose dual solution yields the representation 
</p><div class="math">\[
f(x)=\sum_i^N \tilde{\alpha}_i K(x_i,x)+b,
\]</div><p>
where <span class="math">\(\tilde{\alpha}_i = \alpha_i y_i,\)</span> for <span class="math">\(i=1,2,\ldots, N.\)</span>
</div></p><p>Comparing the above form of kernel machine function with <a href="#ann.kernel.form.eq">(6.8)</a>, we see that neural networks generalize the idea of <b>kernel machines</b> by learning both the feature representation and the output model/layer (which is a linear model in our present case).</p><p>From this viewpoint, universal approximation theorems can be reinterpreted as stating that there exists a set of learned features <span class="math">\(\{\phi_j\}_{j=1}^{\hat{n}}\)</span> such that the linear space <span class="math">\(\mathcal{F}:=\text{span}\{\phi_j\}_{j=1}^{\hat{n}}\)</span> is dense (with respect to the uniform norm) in <span class="math">\(\mathbb{C}(\mathcal{X})\)</span> for any compact input domain <span class="math">\(\mathcal{X}\)</span>.</p><p><h3 id="learning-dynamics-and-the-neural-tangent-kernel">Learning dynamics and the Neural Tangent Kernel</h3></p><p>In the previous discussion, we viewed shallow neural networks as linear models in a learned feature space.  
We also saw that kernel machines correspond to linear models in a fixed (possibly infinite-dimensional) feature space determined by a kernel function.  
We now show that, in a certain infinite-width limit, neural networks themselves naturally induce a kernel, called the <b>Neural Tangent Kernel</b> (NTK), which connects learning dynamics in neural networks with kernel regression.</p><p>During training, the parameters are updated by gradient descent update rule
</p><div class="math">\[
\boldsymbol{\Theta}_{k+1} = \boldsymbol{\Theta}_k - \eta \nabla_{\boldsymbol{\Theta}} \mathcal{C}(\boldsymbol{\Theta}_k),
\]</div><p>
for a chosen learning rate <span class="math">\(\eta > 0\)</span>. Each update changes the output function <span class="math">\({\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_k)\)</span>. Linearizing the network function around <span class="math">\(\boldsymbol{\Theta}_k\)</span> for a very small <span class="math">\(\eta>0\)</span>, we get
</p><div class="math">\[
{\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_{k+1}) \approx {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_k)
+ \big(\nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_k)\big) (\boldsymbol{\Theta}_{k+1} - \boldsymbol{\Theta}_k),
\]</div><p>
where <span class="math">\(\nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}}\)</span> denotes the Jacobian of the network output with respect to the parameters.</p><p><div class="note">
<div class="heading-container">
<b class="heading">Note:</b>
</div>
If <span class="math">\({\mathscr{F}_{L,\hat{n}}}\)</span> is a real-valued function (i.e., <span class="math">\(n_L = 1\)</span>), then <span class="math">\(\nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}}\)</span> is a row vector. 
To be consistent with the convention that vectors are represented as column vectors, it is customary to write 
<span class="math">\((\nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}})^\top\)</span> in this case.
</div></p><p>Using the gradient descent update rule, we get
</p><div class="math">\[
{\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_{k+1}) \approx {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_{k})
+ \big(\nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_{k})\big)
	\Big(- \eta \nabla_{\boldsymbol{\Theta}} \mathcal{C}(\boldsymbol{\Theta}_{k})\Big),
\]</div><p>
which can be rewritten as
</p><div class="math">\[
{\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_{k+1}) - {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_{k}) \approx
- \eta \big(\nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_{k})\big)
	\Big(\nabla_{\boldsymbol{\Theta}} \mathcal{C}(\boldsymbol{\Theta}_{k})\Big).
\]</div><p>
Consider a supervised dataset <span class="math">\(\{(\boldsymbol{x}_i, \boldsymbol{y}_i)\}_{i=1}^N\)</span> and the quadratic cost function
</p><div class="math">\[
\mathcal{C}(\boldsymbol{\Theta}) = \frac{1}{2} \sum_{i=1}^N \left({\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}_i; \boldsymbol{\Theta}) - \boldsymbol{y}_i\right)^2.
\]</div><p>
The gradient of the cost function in the parameter space is given by
</p><div class="math">\[
\nabla_{\boldsymbol{\Theta}} \mathcal{C}(\boldsymbol{\Theta}) =  \sum_{i=1}^N \nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}_i; \boldsymbol{\Theta})^\top\, \big({\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}_i; \boldsymbol{\Theta}) - \boldsymbol{y}_i\big).
\]</div><p>
Substituting this expression in the above expression, we get
</p><div class="math">\[
{\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_{k+1}) - {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_{k}) \approx
- \eta \sum_{i=1}^N  \big(\nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_{k})\big) 
\big(\nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}_i; \boldsymbol{\Theta}_{k})\big)^\top
\big({\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}_i; \boldsymbol{\Theta}_{k}) - \boldsymbol{y}_i\big),
\]</div><p>
Define
</p><div class="math">\[
\text{𝕜}_{\hat{n}}(\boldsymbol{x}_1,\boldsymbol{x}_2;\boldsymbol{\Theta}) =  \big(\nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}_1; \boldsymbol{\Theta})\big) 
\big(\nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}_2; \boldsymbol{\Theta})\big)^\top,
\]</div><p>
which is called the <b>Neural Tangent Kernel</b> (NTK).</p><p><div class="note">
<div class="heading-container">
<b class="heading">Note:</b>
</div>
Observe that <span class="math">\(\text{𝕜}_{\hat{n}}(\boldsymbol{x}_1,\boldsymbol{x}_2;\boldsymbol{\Theta})\)</span> is a matrix if <span class="math">\(n_L>1\)</span> and it is a scalar if <span class="math">\(n_L=1\)</span>.
</div></p><p>The change in the value of the neural network function at a point <span class="math">\(\boldsymbol{x}\)</span> due to the update in the parameters is given (up to first order) in terms of NTK as
</p><div class="math">\[
{\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_{k+1}) - {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}_{k}) \approx
- \eta \sum_{i=1}^N  \text{𝕜}_{\hat{n}}(\boldsymbol{x},\boldsymbol{x}_i;\boldsymbol{\Theta})\big({\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}_i; \boldsymbol{\Theta}_{k}) - \boldsymbol{y}_i\big).
\]</div><p>
By taking <span class="math">\(t_{k}=k\eta\)</span> as a discrete time, we can see that the above rule is the explicit Euler discretization of the system of ordinary differential equations
</p><div class="math">\[
\frac{d}{dt} {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x};\boldsymbol{\Theta}(t)) = 
	- \sum_{i=1}^N  \text{𝕜}_{\hat{n}}(\boldsymbol{x},\boldsymbol{x}_i;\boldsymbol{\Theta}(t))\big({\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}_i; \boldsymbol{\Theta}(t)) - \boldsymbol{y}_i\big).
\]</div><p>
Since NTK depends on the gradient of <span class="math">\({\mathscr{F}_{L,\hat{n}}}\)</span>, it follows that <span class="math">\(\text{𝕜}_{\hat{n}}\)</span> depends implicitly on <span class="math">\({\mathscr{F}_{L,\hat{n}}}\)</span> through its parameter <span class="math">\(\boldsymbol{\Theta}(t)\)</span>. Hence, the RHS of the above system is nonlinear in <span class="math">\({\mathscr{F}_{L,\hat{n}}}\)</span>, which means that the above system is a nonlinear system of ODEs.</p><p>By increasing the hidden layer width <span class="math">\(\hat{n}\)</span>, we can show (proof omitted for this course) that for a sufficiently wide network with the standard NTK parameterisation (weights scaled by <span class="math">\(1/\sqrt{\hat n}\)</span>) and random initialization, we have
</p><div class="math">\[
\nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}(t)) \approx 
\nabla_{\boldsymbol{\Theta}} {\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}; \boldsymbol{\Theta}(0)).
\]</div><p>
where the approximation improves as <span class="math">\(\hat n\to\infty\)</span> (in probability, and uniformly for <span class="math">\(t\)</span> in any fixed interval <span class="math">\([0,T]\)</span>), under mild regularity assumptions on the activation.
Consequently the empirical NTK converges to a deterministic, time-invariant kernel <span class="math">\(\text{𝕜}_\infty\)</span> and becomes independent of <span class="math">\(t\)</span>. That is,
</p><div class="math">\[
\text{𝕜}_{\hat n}(\boldsymbol{x},\boldsymbol{x}';\boldsymbol{\Theta}(t)) \xrightarrow[\hat n\to\infty]{} \text{𝕜}_\infty(\boldsymbol{x},\boldsymbol{x}';\boldsymbol{\Theta}(0)).
\]</div><p>
Therefore, in the infinite-width limit the network function evolves (approximately) according to the linear system
</p><div class="math">\[
\frac{d}{dt} {\mathscr{F}_{L,\infty}}(\boldsymbol{x};\boldsymbol{\Theta}(t)) = 
	- \sum_{i=1}^N  \text{𝕜}_{\infty}(\boldsymbol{x},\boldsymbol{x}_i)\big({\mathscr{F}_{L,\infty}}(\boldsymbol{x}_i; \boldsymbol{\Theta}(t)) - \boldsymbol{y}_i\big),
\]</div><p>
making the training dynamics linear in function space.</p><p>Considering the regularized cost function with the <span class="math">\(\ell^2\)</span>-regularization in the function space (rather than in the parameter space) given by 
</p><div class="math">\[
\mathcal{C}(\boldsymbol{\Theta}) = \frac{1}{2} \sum_{i=1}^N \left({\mathscr{F}_{L,\hat{n}}}(\boldsymbol{x}_i; \boldsymbol{\Theta}) - \boldsymbol{y}_i\right)^2 + \frac{\alpha}{2}\|{\mathscr{F}_{L,\hat{n}}}(\cdot; \boldsymbol{\Theta})\|^2,
\]</div><p>
where <span class="math">\(\alpha>0\)</span> is the penalization parameter or ridge coefficient.</p><p>Then, the above system takes the form
</p><div class="math">\[
\frac{d}{dt} {\mathscr{F}_{L,\infty}}(\boldsymbol{x};\boldsymbol{\Theta}(t)) = 
	- \sum_{i=1}^N  \text{𝕜}_{\infty}(\boldsymbol{x},\boldsymbol{x}_i)\big({\mathscr{F}_{L,\infty}}(\boldsymbol{x}_i; \boldsymbol{\Theta}(t)) - \boldsymbol{y}_i\big) - \alpha {\mathscr{F}_{L,\infty}}(\boldsymbol{x}; \boldsymbol{\Theta}(t)).
\]</div><p>
Assume that the sequence <span class="math">\(\{\boldsymbol{\Theta}_k\}\)</span> generated by the gradient descent update rule converges to <span class="math">\(\boldsymbol{\Theta}^*\)</span> as <span class="math">\(k\rightarrow \infty\)</span>. Consequently, as <span class="math">\(t\rightarrow \infty\)</span>, we obtain the steady state <span class="math">\(\dfrac{d{\mathscr{F}_{L,\infty}}}{dt} = 0 \)</span>, which implies
</p><div class="math">\[
{\mathscr{F}_{L,\infty}}(\boldsymbol{x}; \boldsymbol{\Theta}^*) = \frac{1}{\alpha}\sum_{i=1}^N  \text{𝕜}_{\infty}(\boldsymbol{x},\boldsymbol{x}_i)\big( \boldsymbol{y}_i -{\mathscr{F}_{L,\infty}}( \boldsymbol{x}_i; \boldsymbol{\Theta}^*) \big).
\]</div><p>
Taking
</p><div class="math">\[
\boldsymbol{\beta}_i = \frac{\big( \boldsymbol{y}_i -{\mathscr{F}_{L,\infty}}( \boldsymbol{x}_i; \boldsymbol{\Theta}^*) \big)}{\alpha}\in \mathbb{R}^{n_L}, ~i=1,2,\ldots, N,
\]</div><p>
we can write
</p><div class="math">\[
{\mathscr{F}_{L,\infty}}(\boldsymbol{x}; \boldsymbol{\Theta}^*) = \sum_{i=1}^N  \text{𝕜}_{\infty}(\boldsymbol{x},\boldsymbol{x}_i)\boldsymbol{\beta}_i.
\]</div><p>
Let us use the notation 
</p><div class="math">\[
\boldsymbol{\beta}_{\mathrm{vec}} =
\begin{bmatrix}
\boldsymbol{\beta}_1 \\
\boldsymbol{\beta}_2 \\
\vdots \\
\boldsymbol{\beta}_N
\end{bmatrix}
\in\mathbb{R}^{N n_L},
\]</div><p>
where each coordinate <span class="math">\(\boldsymbol{\beta}_i\)</span> is a block column vector, and for the given input set <span class="math">\(\mathcal{X} = \{\boldsymbol{x}_i\}_{i=1}^N,\)</span> let 
</p><div class="math">\[
K_\infty(\mathcal{X},\mathcal{X}) = \big(\text{𝕜}_\infty(\boldsymbol{x}_i, \boldsymbol{x}_j)\big)_{i,j=1}^N
\]</div><p> 
be the <span class="math">\(N\times N\)</span> symmetric block Gram matrix whose <span class="math">\((i,j)\)</span>-th block is the Gram matrix <span class="math">\(\text{𝕜}_\infty(\boldsymbol{x}_i, \boldsymbol{x}_j)\)</span>.  With these notations, we can write
</p><div class="math">\[
{\mathscr{F}_{L,\infty}}(\boldsymbol{x}; \boldsymbol{\Theta}^*) = 
K_\infty(\boldsymbol{x},\mathcal{X}) \boldsymbol{\beta}_{\mathrm{vec}},
\]</div><p>
where
</p><div class="math">\[
K_\infty(\boldsymbol{x},\mathcal{X})
=\big(
	\text{𝕜}_\infty(\mathbf{x},\mathbf{x}_1),
	\text{𝕜}_\infty(\mathbf{x},\mathbf{x}_2),
	\cdots,
	\text{𝕜}_\infty(\mathbf{x},\mathbf{x}_N)
\big)\in 
\mathbb{R}^{n_L\times N n_L}
\]</div><p>
is the block row vector with each component being the <span class="math">\(n_L\times n_L\)</span> matrix.</p><p>The above expression shows that <span class="math">\({\mathscr{F}_{L,\infty}}\)</span> lies in the span of the kernel functions
<span class="math">\(\{ \text{𝕜}_\infty(\cdot, \mathbf{x}_i) \}_{i=1}^N\)</span>, i.e., <span class="math">\({\mathscr{F}_{L,\infty}}\)</span> resembles a kernel machine.</p><p><h2 id="pinns.sec">Physics Informed Neural Networks for ODEs</h2></p><p>Feedforward neural networks can be used to learn solutions of initial and/or boundary value problems for differential equations (ODEs and PDEs).  In this section, we restrict our discussions to initial value problem for first-order ordinary differential equations (ODEs) of the form 
</p><div class="math" id="eq:ivp">\begin{eqnarray}
\begin{aligned}
\frac{dy}{dt} &amp;= f\big(t,y\big), \qquad t\in[0,T],\\
y(0) &amp;= y_0,
\end{aligned}
\end{eqnarray}<div style="text-align:right;">(6.10)</div></div><p>
where <span class="math">\(y=y(t)\)</span> is the solution for a given (sufficiently regular) function <span class="math">\(f:[0,T]\times\mathbb{R}\to\mathbb{R}\)</span>, for some <span class="math">\(T>0\)</span>,  and \(y_0\in\mathbb{R}\) is the prescribed initial value. We seek an approximation <span class="math">\(\widehat{y}(t)\approx y(t)\)</span> for <span class="math">\(t\in[0,T].\)</span></p><p>We briefly discuss how a feedforward neural network (FNN) can be used to obtain an approximation <span class="math">\(\widehat{y}(\cdot)={\mathscr{F}_{L,\hat{n}}}(\cdot;\boldsymbol{\Theta})\)</span>. </p><p>The mapping <span class="math">\(t\mapsto {\mathscr{F}_{L,\hat{n}}}(t;\boldsymbol{\Theta})\)</span> is chosen smooth enough so that time derivatives of the network output are defined via automatic differentiation.</p><p><h3 id="trial-solution">Trial Solution</h3></p><p>There are two ways that we can define the trial solution.</p><p><b>Hard-constrained Formulation:</b></p><p>To enforce the initial condition exactly, define the <b>trial solution</b>
</p><div class="math" id="eq:trial">\begin{eqnarray}
\widetilde{y}(t;\boldsymbol{\Theta}) := y_0 + t\,{\mathscr{F}_{L,\hat{n}}}(t;\boldsymbol{\Theta}).
\end{eqnarray}<div style="text-align:right;">(6.11)</div></div><p>
By construction \(\widetilde{y}(0;\boldsymbol{\Theta})=y_0\) for every choice of parameters <span class="math">\(\boldsymbol{\Theta}\)</span>. Therefore, the neural network function <span class="math">\({\mathscr{F}_{L,\hat{n}}}(t;\boldsymbol{\Theta})\)</span> is the corrective term scaled by <span class="math">\(t\)</span>, and <span class="math">\(\widetilde{y}\)</span> is the required approximate solution of the given IVP which satisfies the initial condition exactly.</p><p><b>Soft-constrained Formulation:</b></p><p>Here, the trial solution itself is represented by the neural network function
</p><div class="math" id="eq:trial">\begin{eqnarray}
\widetilde{y}(t;\boldsymbol{\Theta}) :={\mathscr{F}_{L,\hat{n}}}(t;\boldsymbol{\Theta})
\end{eqnarray}<div style="text-align:right;">(6.12)</div></div><p>
and the initial condition is not enforced exactly rather imposed through a penalty term in the cost function. Thus, in this case, the neural network function is the required approximate solution of the given IVP which satisfies the initial condition approximately.</p><p><h3 id="residual-and-training-loss">Residual and Training Loss</h3></p><p>Define the pointwise <b>residual loss function</b> of the ODE for the trial solution as
</p><div class="math" id="eq:residualloss">\begin{eqnarray}
\mathcal{R}(t;\boldsymbol{\Theta}) := \frac{d}{dt}\widetilde{y}(t;\boldsymbol{\Theta}) - f\big(t,\widetilde{y}(t;\boldsymbol{\Theta})\big).
\end{eqnarray}<div style="text-align:right;">(6.13)</div></div><p>Select a set of <b>collocation points</b> <span class="math">\(\{t_j\}_{j=0}^N \subset [0,T]\)</span> with <span class="math">\(t_0=0\)</span> and <span class="math">\(\Delta t_j = t_j-t_{j-1}\)</span>, <span class="math">\(j=1,2,\ldots, N\)</span>. The mean squared <b>residual cost function</b> is defined as
</p><div class="math" id="eq:residualcost">\begin{eqnarray}
\mathcal{C}_{\texttt{res}}(\boldsymbol{\Theta}) \;=\; \sum_{i=1}^N \big|\mathcal{R}(t_i;\boldsymbol{\Theta})\big|^2\Delta t_i.
\end{eqnarray}<div style="text-align:right;">(6.14)</div></div><p>
The collocation points may be chosen uniformly, randomly, or using adaptive strategies. The residual cost function ensures that the neural network satisfies the ODE <a href="#eq:ivp">(6.10)</a> approximately.  We also need to enforce the initial condition <span class="math">\(y(0)=y_0\)</span>, which can be done by adding a penalty corresponding to the initial condition as
</p><div class="math" id="eq:penalty">\begin{eqnarray}
\mathcal{L}_{\texttt{ic}}(\boldsymbol{\Theta}) = \left|\widetilde{y}(0;\boldsymbol{\Theta})  - y_0 \right|^2.
\end{eqnarray}<div style="text-align:right;">(6.15)</div></div><p>Finally, the <b>total cost function</b> is given by
</p><div class="math" >\begin{eqnarray}
\mathcal{C}(\boldsymbol{\Theta}) = \mathcal{C}_{\texttt{res}}(\boldsymbol{\Theta}) + \lambda\, \mathcal{L}_{\texttt{ic}}(\boldsymbol{\Theta}),
\end{eqnarray}<div style="text-align:right;">(6.16)</div></div><p>
where <span class="math">\(\lambda > 0\)</span> is a weighting parameter controlling the relative importance of satisfying the initial condition.</p><p><div class="remark">
<div class="heading-container">
<b class="heading">Remark:</b>
</div>
</p><p>Differentiating <span class="math">\(\widetilde{y}(t;\boldsymbol{\Theta})\)</span> in the residual loss function is equivalent to differentiating the neural network function <span class="math">\({\mathscr{F}_{L,\hat{n}}}(t;\boldsymbol{\Theta})\)</span>, which we need to compute as a part of the optimization method.  There are two ways that we can compute <span class="math">\(\frac{d}{dt}{\mathscr{F}_{L,\hat{n}}}(t;\boldsymbol{\Theta})\)</span>.
<ol class="latex-enumerate">
</li><li>We can perform exact computation of <span class="math">\(\frac{d}{dt}{\mathscr{F}_{L,\hat{n}}}(t;\boldsymbol{\Theta})\)</span> using automatic differentiation (not covered in this course) of the FNN.
</li><li>We can also approximate <span class="math">\(\frac{d}{dt}\widetilde{y}(t;\boldsymbol{\Theta})\)</span> at the collocation points using backward difference operator given by
</p><div class="math">\[
\frac{d}{dt}\widetilde{y}(t;\boldsymbol{\Theta}) \approx \frac{\widetilde{y}(t_{k};\boldsymbol{\Theta}) - \widetilde{y}(t_{k-1};\boldsymbol{\Theta})}{\Delta t_i}.
\]</div><p>
</li></ol>
</div></p><p><div class="note">
<div class="heading-container">
<b class="heading">Note:</b>
</div>
The ANN-based ODE solver discussed here is a special case of the well-known <b>Physics-Informed Neural Networks</b> (PINN). The essence of PINNs is to incorporate the underlying physics governed by the differential equation through the residual cost function, along with the associated initial and/or boundary conditions.
</div></p><p>Extending the ideas of PINNs from ODEs to PDEs is not a straight forward task.  However, in certain simple PDE problems, the PINN formulation is simple.</p><p><div class="problem">
<div class="heading-container">
<b class="heading">Problem:</b>
</div>
Obtain the total cost function for the soft-constrained formulation of the following initial and value problem:
</p><div class="math">\[
\begin{array}{cc}
\dfrac{\partial u}{\partial t} + \dfrac{\partial u}{\partial x} = 0,&amp;(x,t)\in [a,b]\times [0,T],\\
u(x,0) = u_0(x),&amp;x\in [a,b],\\
u(a,t) = u_0(a), &amp; t\in[0,T].
\end{array}
\]</div><p>
</div></p><p><div class="hint">
<div class="heading-container">
<b class="heading">Hint:</b>
</div>
<li>First write the trial function <span class="math">\(\tilde{u}(x,t;\boldsymbol{\theta})\)</span>.
</li><li>Write the PDE residual loss function.
</li><li>Write the loss functions for initial and boundary conditions, separately.
</li><li>Introduce the two dimensional collocation points for <span class="math">\([a,b]\times [0,T]\)</span>.
</li><li>Write the PDE residual cost function on the collocation points (with double sum).
</li><li>Write the initial and boundary cost functions on the collocation points (separately, in space and time variables).
</li><li>Finally, write the total cost function.
</div></p><p><h3 id="training-algorithm">Training Algorithm</h3></p><p>The training goal is to find a parameter set <span class="math">\(\boldsymbol{\Theta}^\star\)</span> such that 
</p><div class="math">\[
\boldsymbol{\Theta}^\star = \text{argmin}_{\boldsymbol{\Theta}} \, \mathcal{C}(\boldsymbol{\Theta}).
\]</div><p> 
As an exact solution cannot be obtained for the above unconstrained optimization problem, we may use any of the standard gradient-based optimizers (like mini-batch GD, SGD with momentum or Adam) to obtain an approximation to <span class="math">\(\boldsymbol{\Theta}^\star\)</span>.</p><p><div class="algorithm">
<div class="heading-container">
<b class="heading">Algorithm:</b>
</div>
</p><p><b>Input:</b>

    </li><li>Choose FNN architecture for <span class="math">\({\mathscr{F}_{L,\hat{n}}}\)</span> (number of layers, neurons, activation functions).
    </li><li>Select collocation points \(\{t_j\}_{j=0}^N\) in \([0,T]\).
    </li><li>Choose an initial parameter set <span class="math">\(\boldsymbol{\Theta}_0\)</span>.
</p><p><b>Processing:</b></p><p>For <span class="math">\(k=1,2,\ldots\)</span>, do the following steps:
<ol class="latex-enumerate">
    </li><li>Form the trial solution \(\widetilde{y}(t_k;\boldsymbol{\Theta}_k)=y_0+t_k\,{\mathscr{F}_{L,\hat{n}}}(t_k;\boldsymbol{\Theta}_k)\).
    </li><li>Compute residuals \(R(t_j;\boldsymbol{\Theta}_k)\), <span class="math">\(j=1,2,\ldots, N\)</span>, at collocation points using <a href="#eq:residualloss">(6.13)</a>.
    </li><li>Compute the residual cost \(\mathcal{L}(\boldsymbol{\Theta}_k)\) using <a href="#eq:residualcost">(6.14)</a>.
    </li><li>Compute the penalty term <span class="math">\(\mathcal{L}_{\texttt{ic}}(0;\boldsymbol{\Theta}_k)\)</span> using <a href="#eq:penalty">(6.15)</a>
    </li><li>Update <span class="math">\(\boldsymbol{\Theta}_k\)</span> by gradient-based optimisation. 
    </li><li>Check for the regularization condition. If satisfied, break the process. If not, then increate <span class="math">\(k\)</span> by one and repeat the above steps.
</li></ol></p><p><b>Output:</b> Take <span class="math">\(\boldsymbol{\Theta}^\star = \boldsymbol{\Theta}_k\)</span>, which the parameter set returned by the optimization method. Return \(\widetilde{y}(t;\boldsymbol{\Theta}^\star)\) as the approximate solution.</p><p>
</div></p><p><div class="code">
<div class="heading-container">
<b class="heading">Code:</b>
</div>
The following code illustrate the training of a PINN: </p><p>
<a href="https://colab.research.google.com/drive/1S5w6tPfObvabakS0PvJ9yn21HQNB7TAH?usp=sharing">
PINNLinearAdvection
</a>
</p><p>The code is developed using PyTorch. Go through the code carefully and understand it.</p><p>The same can be developed using TensorFlow.</p><p> The output of the code is shown below, where the PINN solution is compared with the exact solution of the given initial and boundary value problem for the linear advection equation.
</div></p><p>
<iframe
  src="Figures/pinn_advection.html"
  width="900"
  height="600"
  style="border: none;">
</iframe>
<h2 id="research-perspectives">Research Perspectives</h2></p><p>Developing suitable architectures and efficient learning procedures for PINNs and their variants is an active research area. They form an important part of a growing branch of Machine Learning, known as <b>Scientific Machine Learning</b> (SciML). The importance of SciML lies in its wide range of applications, such as</p><p>
<li>fluid mechanics (incompressible and compressible Navier–Stokes equations, and turbulence modeling),
</li><li>solid mechanics and material modeling,
</li><li>inverse problems (parameter identification),
</li><li>computational finance (stochastic differential equations, option pricing, and portfolio optimization formulated via stochastic control or Hamilton–Jacobi–Bellman equations),
</li><li>Quantum mechanics (Schrödinger-type equations),
</p><p>and so on.</p><p>The development and analysis of PINNs and their variants broadly goes through the following steps: 

</li><li>identifying the appropriate functional setting (solution space) for the giving problem (initial and/or boundary value problem),
</li><li>designing a suitable architecture to learn an approximate solution of the problem,
</li><li>analyzing the expressivity of the network class (similar to universal approximation results) in the solution space,
</li><li>devising efficient and stable learning methodologies, and
</li><li>performing numerical experiments to demonstrate the performance of the learned model, and
</li><li>performing error analyses (theoretically and numerically) to show how well the trained neural network approximates a solution of the problem.
</p><p>At present, there is active research towards using the Neural Tangent Kernel (NTK) framework to analyze, and in some cases improve, the training dynamics and generalization properties of various types of neural networks, including transformers (to a certain extent), and in particular, of PINNs.  </p><p>The NTK perspective is mainly used in analyzing the convergence behavior, spectral bias, and optimization dynamics of neural networks. Thus, NTK may be used to bridge the gap between theoretical analysis and practical implementation.
</p>
    </div>

<footer>
  © S. Baskar, Department of Mathematics, IIT Bombay. 2025 — Updated: 21-Oct-2025
</footer>

<style>
footer {
  position: fixed;
  bottom: 0;
  left: 0;
  width: 100%;
  background: #f8f8f8;
  color: #888;
  font-size: 0.75em;
  text-align: center;
  padding: 6px 0;
  border-top: 1px solid #ddd;
  z-index: 10;               /* low z-index */
  pointer-events: none;      /* allows clicks to pass through */
}

/* Small screen adjustments */
@media (max-width: 600px) {
  footer {
    font-size: 0.68em;
    padding: 4px 0;
  }
}
</style>


    <script>
    document.addEventListener("contextmenu", function(e) { e.preventDefault(); });
    document.addEventListener("keydown", function(e) {
        if ((e.ctrlKey || e.metaKey) && (e.key === "p" || e.key === "s")) {
            e.preventDefault();
        }
    });
    </script>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const tocButton = document.getElementById('toc-button');
            const tocPanel = document.getElementById('toc-panel');
            const closeToc = document.getElementById('close-toc');

            if (localStorage.getItem('tocOpen') === 'true') {
                tocPanel.classList.add('open');
                localStorage.removeItem('tocOpen');
            }

            tocButton.addEventListener('click', function(e) {
                e.stopPropagation();
                tocPanel.classList.toggle('open');
            });

            function closeTocPanel() {
                tocPanel.classList.remove('open');
            }

            closeToc.addEventListener('click', function(e) {
                e.stopPropagation();
                closeTocPanel();
            });

            document.addEventListener('click', function(e) {
                if (!tocPanel.contains(e.target) && e.target !== tocButton) {
                    closeTocPanel();
                }
            });

            tocPanel.addEventListener('click', function(e) {
                e.stopPropagation();
            });

            document.querySelectorAll('#toc-panel a').forEach(link => {
                link.addEventListener('click', function(e) {
                    if (!this.getAttribute('href').startsWith('#')) {
                        localStorage.setItem('tocOpen', 'true');
                    }
                    if (this.hash) {
                        e.preventDefault();
                        const target = document.getElementById(this.hash.substring(1));
                        if (target) {
                            window.scrollTo({
                                top: target.offsetTop - 70,
                                behavior: 'smooth'
                            });
                            history.pushState(null, null, this.hash);
                            if (window.innerWidth < 768) {
                                closeTocPanel();
                            }
                        }
                    }
                });
            });

            if (window.location.hash) {
                const target = document.getElementById(window.location.hash.substring(1));
                if (target) {
                    setTimeout(() => {
                        window.scrollTo({
                            top: target.offsetTop - 70,
                            behavior: 'smooth'
                        });
                    }, 100);
                }
            }
        });
    </script>
</body>
</html>
